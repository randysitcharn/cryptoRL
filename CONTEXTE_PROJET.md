# CONTEXTE PROJET - CryptoRL

> **Projet:** Reinforcement Learning pour trading de cryptomonnaies  
> **DerniÃ¨re mise Ã  jour:** 2026-01-17

---

## ğŸš€ DÃ©marrage rapide

### Environnement virtuel

Le projet utilise un environnement virtuel Python. Par dÃ©faut, il se trouve dans `.venv/` Ã  la racine du projet.

**CrÃ©er l'environnement virtuel (si nÃ©cessaire):**
```bash
python -m venv .venv
```

**Activer l'environnement virtuel:**

- **Windows (PowerShell):**
  ```powershell
  .\.venv\Scripts\Activate.ps1
  ```

- **Windows (CMD):**
  ```cmd
  .venv\Scripts\activate.bat
  ```

- **Linux/Mac:**
  ```bash
  source .venv/bin/activate
  ```

**Installer les dÃ©pendances:**
```bash
pip install -r requirements.txt
```

---

## ğŸ“ Structure du projet

```
cryptoRL/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed/              # DonnÃ©es traitÃ©es (parquet)
â”‚   â”œâ”€â”€ raw/                   # DonnÃ©es brutes
â”‚   â””â”€â”€ raw_historical/         # DonnÃ©es historiques OHLCV
â”œâ”€â”€ docs/                      # Documentation
â”œâ”€â”€ logs/                      # Logs d'entraÃ®nement
â”œâ”€â”€ results/                    # RÃ©sultats et visualisations
â”œâ”€â”€ scripts/                   # Scripts d'exÃ©cution
â”‚   â””â”€â”€ run_full_wfo.py        # Script principal WFO
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config/                # Configuration
â”‚   â”œâ”€â”€ data/                   # Chargement des donnÃ©es
â”‚   â”œâ”€â”€ data_engineering/      # Feature engineering (FFD, HMM, etc.)
â”‚   â”œâ”€â”€ evaluation/            # Ã‰valuation et backtesting
â”‚   â”œâ”€â”€ models/                # ModÃ¨les (MAE, TQC)
â”‚   â”œâ”€â”€ training/              # Infrastructure d'entraÃ®nement
â”‚   â””â”€â”€ utils/                 # Utilitaires
â””â”€â”€ tests/                     # Tests unitaires
```

---

## ğŸ”‘ Fichiers principaux

| Fichier | Description |
|---------|-------------|
| `scripts/run_full_wfo.py` | Orchestration WFO complÃ¨te (HMM â†’ MAE â†’ TQC â†’ Ã‰valuation) |
| `src/training/train_agent.py` | EntraÃ®nement TQC avec modÃ¨le Foundation |
| `src/training/batch_env.py` | Environnement vectorisÃ© GPU/CPU |
| `src/models/foundation.py` | ModÃ¨le MAE (autoencodeur) |
| `src/models/rl_adapter.py` | Adaptateur Foundation â†’ TQC |

---

## ğŸ—ï¸ Architecture

```
WFO Pipeline (run_full_wfo.py)
â”œâ”€â”€ [1] Chargement donnÃ©es (CSV/Parquet)
â”œâ”€â”€ [2] Feature engineering (FFD, Z-Score, Parkinson, Garman-Klass)
â”œâ”€â”€ [3] DÃ©tection rÃ©gimes HMM (4 Ã©tats)
â”œâ”€â”€ [4] Pre-training MAE (90 epochs)
â”œâ”€â”€ [5] EntraÃ®nement TQC (BatchCryptoEnv, 54M steps)
â””â”€â”€ [6] Ã‰valuation OOS (backtest fenÃªtre test)
```

**Environnement d'entraÃ®nement:**
- `BatchCryptoEnv` (batch_env.py) - GPU/CPU, supporte n_envs=1 pour Ã©valuation

**Callbacks:**
- `ThreePhaseCurriculumCallback` - Curriculum learning (3 phases)
- `RotatingCheckpointCallback` - Optimisation disque
- `TrainingMetricsCallback` - Logging NAV mode WFO

---

## âš™ï¸ Configuration

### ParamÃ¨tres WFO

| ParamÃ¨tre | Valeur |
|-----------|--------|
| train_months | 12 (8,640 lignes) |
| test_months | 3 (2,160 lignes) |
| step_months | 3 (2,160 lignes) |

### EntraÃ®nement

| ParamÃ¨tre | Valeur |
|-----------|--------|
| tqc_timesteps | 30,000,000 |
| mae_epochs | 90 |
| n_envs | 1024 |
| batch_size | 2048 |
| learning_rate | 1e-4 |
| gamma | 0.95 |

### Curriculum (3 phases)

| Phase | Progression | Churn | Smooth |
|-------|-------------|-------|--------|
| 1 - Discovery | 0-10% | 0.0 â†’ 0.10 | 0.0 |
| 2 - Discipline | 10-30% | 0.10 â†’ 0.50 | 0.0 â†’ 0.02 |
| 3 - Consolidation | 30-100% | 0.50 (fixe) | 0.02 (fixe) |

---

## ğŸ–¥ï¸ Serveur distant

| PropriÃ©tÃ© | Valeur |
|-----------|--------|
| Host | `158.51.110.52` |
| Port | `20941` |
| User | `root` |
| Provider | vast.ai |
| TensorBoard | Port 8081 |

**Connexion:**
```bash
ssh -p 20941 root@158.51.110.52

# Tunnel TensorBoard
ssh -p 20941 -L 8081:localhost:8081 root@158.51.110.52
```

---

## ğŸ“Š Architecture MORL

Le projet utilise une architecture MORL (Multi-Objective Reinforcement Learning) pour gÃ©rer l'Ã©quilibre entre performance et coÃ»ts:

```python
reward = r_perf + curriculum_lambda * w_cost * r_cost * MAX_PENALTY_SCALE
```

oÃ¹:
- `r_perf`: Log-returns (objectif performance)
- `w_cost âˆˆ [0, 1]`: ParamÃ¨tre MORL dans l'observation
- `curriculum_lambda âˆˆ [0, 0.4]`: Progression contrÃ´lÃ©e

---

*Document simplifiÃ© - Pour plus de dÃ©tails, voir la documentation dans `docs/`*
