# CONTEXTE PROJET - CryptoRL

> **Projet:** Reinforcement Learning pour trading de cryptomonnaies  
> **DerniÃ¨re mise Ã  jour:** 2026-01-17

---

## ğŸš€ DÃ©marrage rapide

### Environnement virtuel

Le projet utilise un environnement virtuel Python. Par dÃ©faut, il se trouve dans `venv/` Ã  la racine du projet.

**CrÃ©er l'environnement virtuel (si nÃ©cessaire):**
```bash
python -m venv venv
```

**Activer l'environnement virtuel:**

- **Windows (PowerShell):**
  ```powershell
  venv\Scripts\activate
  ```

- **Windows (CMD):**
  ```cmd
  venv\Scripts\activate.bat
  ```

- **Linux/Mac:**
  ```bash
  source venv/bin/activate
  ```

**Installer les dÃ©pendances:**
```bash
pip install -r requirements.txt
```

---

## ğŸ“ Structure du projet

```
cryptoRL/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed/              # DonnÃ©es traitÃ©es (parquet)
â”‚   â”œâ”€â”€ raw/                   # DonnÃ©es brutes
â”‚   â””â”€â”€ raw_historical/         # DonnÃ©es historiques OHLCV
â”œâ”€â”€ docs/                      # Documentation
â”œâ”€â”€ logs/                      # Logs d'entraÃ®nement
â”œâ”€â”€ results/                    # RÃ©sultats et visualisations
â”œâ”€â”€ scripts/                   # Scripts d'exÃ©cution
â”‚   â””â”€â”€ run_full_wfo.py        # Script principal WFO
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config/                # Configuration
â”‚   â”œâ”€â”€ data/                   # Chargement des donnÃ©es
â”‚   â”œâ”€â”€ data_engineering/      # Feature engineering (FFD, HMM, etc.)
â”‚   â”œâ”€â”€ evaluation/            # Ã‰valuation et backtesting
â”‚   â”œâ”€â”€ models/                # ModÃ¨les (MAE, TQC)
â”‚   â”œâ”€â”€ training/              # Infrastructure d'entraÃ®nement
â”‚   â””â”€â”€ utils/                 # Utilitaires
â””â”€â”€ tests/                     # Tests unitaires
```

---

## ğŸ”‘ Fichiers principaux

| Fichier | Description |
|---------|-------------|
| `scripts/run_full_wfo.py` | Orchestration WFO complÃ¨te (HMM â†’ MAE â†’ TQC â†’ Ã‰valuation) |
| `src/training/train_agent.py` | EntraÃ®nement TQC avec modÃ¨le Foundation |
| `src/training/batch_env.py` | Environnement vectorisÃ© GPU/CPU |
| `src/models/foundation.py` | ModÃ¨le MAE (autoencodeur) |
| `src/models/rl_adapter.py` | Adaptateur Foundation â†’ TQC |

---

## ğŸ—ï¸ Architecture

```
WFO Pipeline (run_full_wfo.py)
â”œâ”€â”€ [1] Chargement donnÃ©es (CSV/Parquet)
â”œâ”€â”€ [2] Feature engineering (FFD, Z-Score, Parkinson, Garman-Klass)
â”œâ”€â”€ [3] DÃ©tection rÃ©gimes HMM (4 Ã©tats)
â”œâ”€â”€ [4] Pre-training MAE (90 epochs)
â”œâ”€â”€ [5] EntraÃ®nement TQC (BatchCryptoEnv, 54M steps)
â””â”€â”€ [6] Ã‰valuation OOS (backtest fenÃªtre test)
```

**Environnement d'entraÃ®nement:**
- `BatchCryptoEnv` (batch_env.py) - GPU/CPU, supporte n_envs=1 pour Ã©valuation

**Callbacks:**
- `MORLCurriculumCallback` - Curriculum learning progressif (modulation w_cost)
- `ThreePhaseCurriculumCallback` - âš ï¸ OBSOLETE (remplacÃ© par MORLCurriculumCallback)
- `RotatingCheckpointCallback` - Optimisation disque
- `UnifiedMetricsCallback` - Logging TensorBoard unifiÃ©

---

## âš™ï¸ Configuration

### ParamÃ¨tres WFO

| ParamÃ¨tre | Valeur |
|-----------|--------|
| train_months | 12 (8,640 lignes) |
| test_months | 3 (2,160 lignes) |
| step_months | 3 (2,160 lignes) |

### EntraÃ®nement

| ParamÃ¨tre | Valeur |
|-----------|--------|
| tqc_timesteps | 30,000,000 |
| mae_epochs | 90 |
| n_envs | 1024 |
| batch_size | 2048 |
| learning_rate | 1e-4 |
| gamma | 0.95 |

### Curriculum (MORL)

| Phase | Progression | w_cost | Description |
|-------|-------------|--------|-------------|
| Rampe | 0-50% | 0.0 â†’ 0.1 | Introduction progressive des coÃ»ts |
| Plateau | 50-100% | 0.1 (fixe) | Stabilisation |

**Note:** L'ancien systÃ¨me `ThreePhaseCurriculumCallback` (curriculum_lambda) est obsolÃ¨te. Le nouveau systÃ¨me `MORLCurriculumCallback` module directement `w_cost` dans l'observation (architecture MORL).

---

## ğŸ–¥ï¸ Serveur distant

| PropriÃ©tÃ© | Valeur |
|-----------|--------|
| Host | `86.127.245.129` |
| Port | `25083` |
| User | `root` |
| Provider | vast.ai |
| TensorBoard | Port 8081 |

**Connexion:**
```bash
ssh -p 25083 root@86.127.245.129

# Tunnel TensorBoard
ssh -p 25083 -L 8081:localhost:8081 root@86.127.245.129
```

---

## ğŸ“Š Architecture MORL

Le projet utilise une architecture MORL (Multi-Objective Reinforcement Learning) pour gÃ©rer l'Ã©quilibre entre performance et coÃ»ts:

```python
reward = r_perf + w_cost * r_cost * MAX_PENALTY_SCALE
```

oÃ¹:
- `r_perf`: Log-returns (objectif performance)
- `w_cost âˆˆ [0, 1]`: ParamÃ¨tre MORL dans l'observation (modulÃ© par `MORLCurriculumCallback`)
- `MAX_PENALTY_SCALE = 0.4`: Facteur d'Ã©chelle des pÃ©nalitÃ©s

**Curriculum Learning:**
- `MORLCurriculumCallback` module progressivement `w_cost` de 0.0 (performance pure) Ã  0.1 (Ã©quilibrÃ©) sur 50% du training
- L'agent apprend d'abord Ã  maximiser la performance, puis Ã  Ã©quilibrer avec les coÃ»ts

---

*Document simplifiÃ© - Pour plus de dÃ©tails, voir la documentation dans `docs/`*
