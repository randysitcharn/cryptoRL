# Rapport d'Audit : Corrections Propos√©es pour CryptoRL WFO

**Date** : 21 janvier 2026  
**Version** : 1.0  
**Objectif** : Document destin√© √† l'audit par un LLM externe

---

## 1. Contexte et Diagnostic

### 1.1 R√©sultats WFO Actuels

| Segment | Sharpe | PnL % | DD % | Trades | Alpha vs B&H | March√© | Status |
|---------|--------|-------|------|--------|--------------|--------|--------|
| 0 | -2.95 | -32.1% | 38.3% | 228 | **+4.6%** | üî¥ BEAR | SUCCESS |
| 1 | +3.09 | +35.3% | 10.1% | 149 | **-3.8%** | üü¢ BULL | RECOVERED |
| 2 | -4.84 | -63.1% | 71.3% | **969** | **-182%** | üü¢ BULL | SUCCESS |
| 3 | -0.95 | -11.1% | 29.3% | 438 | **+6.8%** | üî¥ BEAR | SUCCESS |
| 4 | -3.86 | -43.4% | 49.8% | **883** | **-43%** | ‚ö™ RANGE | SUCCESS |

### 1.2 M√©triques TensorBoard Cl√©s (Extraites du Serveur)

| Segment | Position Moyenne | Action Saturation | Entropy | Churn Multiplier PLO |
|---------|------------------|-------------------|---------|----------------------|
| 0 | +0.54 (LONG) | 0.18 ‚Üí 0.24 | 0.078 ‚Üí 0.020 | **1.0** (jamais actif) |
| 2 | +0.41 (LONG) | 0.34 ‚Üí **0.47** | **0.015** | **1.0** (jamais actif) |
| 4 | -0.46 (SHORT) | 0.38 ‚Üí 0.43 | 0.021 | **1.0** (jamais actif) |

### 1.3 Probl√®mes Identifi√©s

#### Probl√®me 1 : Mismatch Train/Eval sur le Volatility Scaling

**Fichier** : `scripts/run_full_wfo.py`, ligne 732

```python
# Actuel (ligne 732)
max_leverage=1.0,  # Disable vol scaling (was: self.config.max_leverage)
```

**Impact** :
- En training : `max_leverage=2.0` ‚Üí les positions sont amplifi√©es jusqu'√† 2x via le `vol_scalar`
- En √©valuation : `max_leverage=1.0` ‚Üí les positions sont brutes, sans amplification
- Un agent qui apprend √† faire des ajustements de 0.05 en training (amplifi√©s √† 0.10) se retrouve √† faire 0.05 en √©valuation, cr√©ant un churn **diff√©rent** de celui appris

#### Probl√®me 2 : PLO Churn Jamais Activ√©

**Observation** : `churn_multiplier = 1.0` sur **tous** les segments

**Cause** : Le `turnover_threshold` (0.08 = 8%) n'est jamais d√©pass√© car le calcul de turnover utilise `metric_turnover = avg(current_position_deltas)` qui est proche de 0 en moyenne, m√™me si le nombre de trades en √©valuation est tr√®s √©lev√© (200-900+).

**Fichier** : `src/training/callbacks.py`, lignes 1123-1131

```python
# Actuel
current_deltas = real_env.current_position_deltas  # Shape: (n_envs,)
avg_turnover = current_deltas.mean().item()
```

**Impact** : Le syst√®me PLO Churn est con√ßu pour augmenter la p√©nalit√© quand le turnover d√©passe un seuil, mais ce seuil n'est jamais atteint car :
1. `current_position_deltas` est le delta **instantan√©** (step actuel vs step pr√©c√©dent)
2. En moyenne sur 1024 envs, ce delta est tr√®s petit
3. Le turnover **cumul√©** par √©pisode n'est pas mesur√©

#### Probl√®me 3 : Alpha N√©gatif dans les March√©s Bull

**Segment 2** : March√© BULL (+119% B&H) mais l'agent fait -63% (Alpha = -182%)

**Analyse** :
- Position moyenne = +0.41 ‚Üí L'agent est bien LONG
- Mais 969 trades en 3 mois = **overtrading massif** (~10 trades/jour)
- Chaque trade co√ªte ~0.05% (commission + slippage)
- Co√ªt total ‚âà 48% en frais (969 √ó 0.05%)

**Cause racine** : L'agent n'est pas p√©nalis√© pour l'overtrading car le PLO Churn est inactif.

#### Probl√®me 4 : Entropy Collapse

**Observation** : `ent_coef` descend √† 0.015 (segment 2)

**Impact** : La politique devient quasi-d√©terministe, r√©p√©tant les m√™mes actions sans exploration, ce qui amplifie l'overtrading appris.

---

## 2. Corrections Propos√©es

### 2.1 CORRECTION 1 : Aligner Volatility Scaling Train/Eval

**Fichier** : `scripts/run_full_wfo.py`

**Avant** (ligne 732) :
```python
max_leverage=1.0,  # Disable vol scaling (was: self.config.max_leverage)
```

**Apr√®s** :
```python
max_leverage=self.config.max_leverage,  # Coh√©rence train/eval
```

**Justification** :
- Le volatility scaling est un composant cl√© de la strat√©gie apprise
- Le d√©sactiver en √©valuation cr√©e un mismatch distribution ‚Üí l'agent ne voit pas l'environnement qu'il a appris
- Le commentaire original mentionne "stuck in cash bug" mais ce bug devrait √™tre r√©solu par le `vol_floor` introduit dans `batch_env.py`

**Risques** :
- Si le bug "stuck in cash" r√©appara√Æt, il faudra investiguer `vol_floor` dans `_calculate_volatility`
- Possible augmentation de la variance des r√©sultats en √©valuation

---

### 2.2 CORRECTION 2 : R√©former le Calcul de Turnover pour PLO Churn

**Fichier** : `src/training/callbacks.py`

**Avant** (lignes 1123-1133) :
```python
# TURNOVER MEASUREMENT
current_deltas = real_env.current_position_deltas
avg_turnover = current_deltas.mean().item()

self.turnover_history.append(avg_turnover)
if len(self.turnover_history) > self.prediction_horizon:
    self.turnover_history.pop(0)

# Average turnover over window
metric_turnover = np.mean(self.turnover_history[-20:]) if len(self.turnover_history) >= 20 else avg_turnover
```

**Apr√®s** :
```python
# TURNOVER MEASUREMENT - v2: Cumulative per Episode
current_deltas = real_env.current_position_deltas
sum_turnover = current_deltas.sum().item()  # Somme sur tous les envs (pas moyenne)
num_envs = real_env.num_envs

# Normaliser par le nombre d'envs pour obtenir turnover moyen par env
avg_turnover_per_env = sum_turnover / num_envs

self.turnover_history.append(avg_turnover_per_env)
if len(self.turnover_history) > self.prediction_horizon:
    self.turnover_history.pop(0)

# Turnover cumul√© sur fen√™tre glissante (plus sensible)
metric_turnover = np.sum(self.turnover_history[-20:]) if len(self.turnover_history) >= 20 else np.sum(self.turnover_history)
```

**Alternative** : Mesurer le turnover comme `total_trades / episode_length` √† la fin de chaque √©pisode.

**Justification** :
- Le turnover **instantan√©** moyen est toujours proche de 0 car la plupart des steps n'ont pas de changement de position
- Le turnover **cumul√©** sur une fen√™tre refl√®te mieux le co√ªt r√©el de l'overtrading
- Avec 969 trades sur 2095 steps, le turnover moyen par step est ~0.46, ce qui d√©passerait facilement le seuil de 0.08

**Risques** :
- Changement de s√©mantique du `turnover_threshold` ‚Üí potentiellement recalibrer le seuil
- Le PLO pourrait devenir trop agressif si mal calibr√©

---

### 2.3 CORRECTION 3 : R√©compense Bas√©e sur l'Alpha (Optionnel - Refonte Majeure)

**Fichier** : `src/training/batch_env.py`

**Avant** (lignes 406-410) :
```python
# 1. BASE REWARD: Log Returns (always active)
safe_returns = torch.clamp(step_returns, min=-0.99)
log_returns = torch.log1p(safe_returns) * SCALE
```

**Apr√®s** :
```python
# 1. BASE REWARD: Alpha vs Buy & Hold (excess return)
safe_returns = torch.clamp(step_returns, min=-0.99)

# Market return (B&H = hold 100% long)
market_return = (self.prices[self.current_steps] - self.prices[self.current_steps - 1]) / self.prices[self.current_steps - 1]
market_return = torch.clamp(market_return, min=-0.99)

# Alpha = portfolio return - market return
alpha = safe_returns - market_return
log_alpha = torch.log1p(torch.abs(alpha)) * torch.sign(alpha) * SCALE
```

**Justification** :
- L'objectif explicite est "battre B&H" ‚Üí la r√©compense doit refl√©ter cet objectif
- Avec des log-returns absolus, l'agent peut √™tre r√©compens√© m√™me s'il sous-performe le march√©
- Avec alpha, l'agent est p√©nalis√© pour toute sous-performance vs B&H

**Risques** :
- Changement majeur de la fonction de r√©compense ‚Üí n√©cessite re-tuning complet
- En march√© BEAR, B&H perd ‚Üí l'agent doit aussi perdre moins, ce qui peut encourager le shorting
- N√©cessite une p√©riode de validation plus longue

**Recommandation** : Tester d'abord les corrections 1 et 2 avant cette refonte.

---

### 2.4 CORRECTION 4 : Augmenter les Coefficients de P√©nalit√©

**Fichier** : `scripts/run_full_wfo.py`

**Avant** (lignes 83-84) :
```python
churn_coef: float = 0.5    # Max target apr√®s curriculum (r√©duit)
smooth_coef: float = 1e-5  # Tr√®s bas (curriculum monte √† 0.00005 max)
```

**Apr√®s** :
```python
churn_coef: float = 1.0    # Doubl√© pour p√©naliser l'overtrading
smooth_coef: float = 0.01  # Augment√© 1000x pour lisser les positions
```

**Fichier** : `src/training/callbacks.py`

**Avant** (lignes 619-623) :
```python
PHASES = [
    {'end_progress': 0.1, 'churn': (0.0, 0.10), 'smooth': (0.0, 0.0)},
    {'end_progress': 0.3, 'churn': (0.10, 0.50), 'smooth': (0.0, 0.005)},
    {'end_progress': 1.0, 'churn': (0.50, 0.50), 'smooth': (0.005, 0.005)},
]
```

**Apr√®s** :
```python
PHASES = [
    {'end_progress': 0.05, 'churn': (0.0, 0.20), 'smooth': (0.0, 0.0)},      # Phase 1: 5% (r√©duit)
    {'end_progress': 0.15, 'churn': (0.20, 1.00), 'smooth': (0.0, 0.01)},    # Phase 2: Ramp rapide
    {'end_progress': 1.0, 'churn': (1.00, 1.00), 'smooth': (0.01, 0.01)},    # Phase 3: Max penalties
]
```

**Justification** :
- Le `churn_coef` actuel (0.5) est insuffisant pour contrebalancer les gains de trading fr√©quent
- Le `smooth_coef` (1e-5) est quasi-nul et n'emp√™che pas les changements brusques
- Le curriculum actuel atteint le max seulement √† 30% du training, laissant 70% sans progression

**Risques** :
- Si les p√©nalit√©s sont trop fortes, l'agent pourrait ne plus trader du tout ("flat agent")
- N√©cessite un monitoring du nombre de trades minimum par √©pisode

---

### 2.5 CORRECTION 5 : R√©duire le Nombre de Timesteps

**Fichier** : `scripts/run_full_wfo.py`

**Avant** (ligne 73) :
```python
tqc_timesteps: int = 90_000_000  # 90M steps
```

**Apr√®s** :
```python
tqc_timesteps: int = 30_000_000  # 30M steps (r√©duit pour √©viter overfitting)
```

**Justification** :
- Les logs montrent que l'`action_saturation` monte √† 0.47 vers la fin du training
- L'`entropy` collapse √† 0.015 indique une politique sur-ajust√©e
- Le mod√®le "best" est souvent trouv√© avant 50% du training (signal "RECOVERED")

**Risques** :
- Potentiellement insuffisant pour apprendre des patterns complexes
- √Ä combiner avec early stopping bas√© sur validation

---

## 3. Plan d'Impl√©mentation Recommand√©

### Phase 1 : Corrections Conservatrices (Quick Wins)

1. **CORRECTION 1** : Aligner vol scaling train/eval
2. **CORRECTION 4** : Augmenter `churn_coef` et `smooth_coef`
3. **CORRECTION 5** : R√©duire timesteps √† 30M

**Temps estim√©** : 15 minutes de modification, 8-12h de re-training WFO

### Phase 2 : Correction du PLO Churn

4. **CORRECTION 2** : R√©former le calcul de turnover

**Temps estim√©** : 30 minutes de modification, tests unitaires requis

### Phase 3 : Refonte Reward (Si Phase 1-2 insuffisantes)

5. **CORRECTION 3** : Alpha-based reward

**Temps estim√©** : 2-4h de modification, re-tuning complet n√©cessaire

---

## 4. M√©triques de Succ√®s Post-Correction

| M√©trique | Seuil Minimum | Objectif |
|----------|---------------|----------|
| Alpha moyen sur 5 segments | > -10% | > 0% |
| Sharpe moyen | > 0 | > 1.0 |
| Trades par segment | < 500 | < 200 |
| Action Saturation fin training | < 0.40 | < 0.30 |
| Entropy fin training | > 0.05 | > 0.10 |
| PLO Churn Multiplier activ√© | > 1.0 sur ‚â•1 segment | > 2.0 si violation |

---

## 5. Questions pour l'Auditeur

1. **Sur la Correction 1** : Le mismatch train/eval est-il la cause principale du gap de performance, ou y a-t-il d'autres facteurs (ex: stochasticit√© de l'environnement) ?

2. **Sur la Correction 2** : Le changement de s√©mantique du turnover (instantan√© ‚Üí cumul√©) pourrait-il cr√©er des effets secondaires non anticip√©s dans le PID controller ?

3. **Sur la Correction 3** : L'utilisation de l'alpha comme r√©compense pourrait-elle cr√©er un probl√®me de "moving target" si le march√© change de r√©gime mid-√©pisode ?

4. **Sur la Correction 4** : Les valeurs propos√©es (`churn_coef=1.0`, `smooth_coef=0.01`) sont-elles calibr√©es correctement par rapport au `SCALE=100` de la reward function ?

5. **Architecture** : Le syst√®me PLO actuel (3 contr√¥leurs PID ind√©pendants) est-il adapt√©, ou faudrait-il un contr√¥leur multi-objectif (ex: MORL) ?

---

## 6. Annexes

### 6.1 Code Source Pertinent

**Reward Function** : `src/training/batch_env.py` lignes 363-493  
**Curriculum Phases** : `src/training/callbacks.py` lignes 619-623  
**PLO Churn** : `src/training/callbacks.py` lignes 1045-1205  
**Evaluation** : `scripts/run_full_wfo.py` lignes 700-954

### 6.2 Donn√©es du Serveur

```
SSH: ssh -p 20941 root@158.51.110.52
R√©sultats: /workspace/cryptoRL/results/wfo_results.csv
Logs: /workspace/cryptoRL/logs/wfo/
```

### 6.3 Configuration Actuelle (WFOConfig)

```python
tqc_timesteps: 90_000_000
learning_rate: 1e-4
buffer_size: 2_500_000
n_envs: 1024
batch_size: 512
gamma: 0.95
ent_coef: "auto_0.5"
churn_coef: 0.5
smooth_coef: 1e-5
target_volatility: 0.05
max_leverage: 2.0
observation_noise: 0.01
critic_dropout: 0.1
```
