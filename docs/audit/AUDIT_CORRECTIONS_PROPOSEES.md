# Rapport d'Audit : Corrections ProposÃ©es pour CryptoRL WFO

**Date** : 21 janvier 2026  
**Version** : 3.0 (Post-Validation SOTA)  
**Objectif** : Document de rÃ©fÃ©rence intÃ©grant l'audit externe et validation finale  
**Statut** : ğŸŸ¢ **VALIDÃ‰ POUR IMPLÃ‰MENTATION**

---

## 0. RÃ©sumÃ© ExÃ©cutif (Validation Finale)

### Verdict des Auditeurs

| Auditeur | Verdict | Focus |
|----------|---------|-------|
| Gemini AI | ğŸŸ¡ PIVOT REQUIS | Architecture MORL |
| Claude (Validation) | ğŸŸ¢ VALIDÃ‰ | ImplÃ©mentation Production-Grade |

### Impact EstimÃ©

- **ImmÃ©diat** : RÃ©solution du mismatch volatility scaling â†’ **+60% amÃ©lioration estimÃ©e**
- **Long terme** : CapacitÃ© Ã  changer de profil de risque sans rÃ©entraÃ®nement (MORL)

### DÃ©couverte Critique ("Smoking Gun")

Le **Distributional Shift** causÃ© par `max_leverage=1.0` en Ã©valuation vs `max_leverage=2.0` en training explique mathÃ©matiquement :
- L'overtrading (969 trades segment 2)
- L'effondrement du PnL (-63%)
- L'agent perÃ§oit une "perte de puissance" et compense par la frÃ©quence

---

## 1. Contexte et Diagnostic

### 1.1 RÃ©sultats WFO Actuels

| Segment | Sharpe | PnL % | DD % | Trades | Alpha vs B&H | MarchÃ© | Status |
|---------|--------|-------|------|--------|--------------|--------|--------|
| 0 | -2.95 | -32.1% | 38.3% | 228 | **+4.6%** | ğŸ”´ BEAR | SUCCESS |
| 1 | +3.09 | +35.3% | 10.1% | 149 | **-3.8%** | ğŸŸ¢ BULL | RECOVERED |
| 2 | -4.84 | -63.1% | 71.3% | **969** | **-182%** | ğŸŸ¢ BULL | SUCCESS |
| 3 | -0.95 | -11.1% | 29.3% | 438 | **+6.8%** | ğŸ”´ BEAR | SUCCESS |
| 4 | -3.86 | -43.4% | 49.8% | **883** | **-43%** | âšª RANGE | SUCCESS |

### 1.2 MÃ©triques TensorBoard ClÃ©s (Extraites du Serveur)

| Segment | Position Moyenne | Action Saturation | Entropy |
|---------|------------------|-------------------|---------|
| 0 | +0.54 (LONG) | 0.18 â†’ 0.24 | 0.078 â†’ 0.020 |
| 2 | +0.41 (LONG) | 0.34 â†’ **0.47** | **0.015** |
| 4 | -0.46 (SHORT) | 0.38 â†’ 0.43 | 0.021 |

### 1.3 Analyse DÃ©taillÃ©e des Positions (Nouvelles DonnÃ©es)

| Segment | Pos Moy | Std | Long% | Short% | Flat% | DÃ©butâ†’Fin | MarchÃ© |
|---------|---------|-----|-------|--------|-------|-----------|--------|
| 0 | **+0.54** | 0.31 | 79.8% | 1.4% | 18.8% | +0.01 â†’ +0.70 | ğŸ”´ BEAR |
| 1 | **+0.69** | 0.10 | 100% | 0% | 0% | +0.72 â†’ +0.63 | ğŸŸ¢ BULL |
| 2 | **+0.41** | 0.34 | 72.6% | 6.7% | 20.7% | **+0.81 â†’ -0.11** | ğŸŸ¢ BULL |
| 3 | **-0.53** | 0.13 | 0% | 99.7% | 0.3% | -0.52 â†’ -0.53 | ğŸ”´ BEAR |
| 4 | **-0.46** | 0.21 | 0% | 99.9% | 0.1% | -0.82 â†’ -0.27 | âšª RANGE |

**Observation Critique (Segment 2)** : La position passe de +0.81 (LONG) Ã  -0.11 (quasi-SHORT) alors que le marchÃ© reste BULL (+119%). C'est la signature d'un **overfitting sur les donnÃ©es de dÃ©but** ou d'une **instabilitÃ© induite par le curriculum**.

### 1.4 Composantes de Reward (Training)

| Segment | PnL Component | Churn Cost | Downside Risk | Smoothness |
|---------|--------------|------------|---------------|------------|
| 0 | +0.023 | -0.000097 | -0.0066 | **-0.150** |
| 1 | +0.054 | -0.000065 | -0.0048 | -0.055 |
| 2 | +0.033 | -0.000049 | -0.0019 | -0.045 |
| 3 | +0.021 | -0.000057 | -0.0021 | **-0.105** |
| 4 | +0.031 | -0.000063 | -0.0026 | **-0.092** |

**Constat** : La **smoothness penalty** domine (10-150x le churn_cost). Le ratio smoothness/pnl atteint 6.5x sur segment 0.

### 1.5 ProblÃ¨mes IdentifiÃ©s

#### ProblÃ¨me 1 : Mismatch Train/Eval sur le Volatility Scaling

**Fichier** : `scripts/run_full_wfo.py`, ligne 732

```python
# Actuel (ligne 732)
max_leverage=1.0,  # Disable vol scaling (was: self.config.max_leverage)
```

**Impact** :
- En training : `max_leverage=2.0` â†’ les positions sont amplifiÃ©es jusqu'Ã  2x via le `vol_scalar`
- En Ã©valuation : `max_leverage=1.0` â†’ les positions sont brutes, sans amplification
- Mismatch de distribution P(s,a) entre train et eval

#### ProblÃ¨me 2 : Churn Non ContrÃ´lÃ© (RÃ©solu avec MORL)

**Observation historique** : L'ancien systÃ¨me PLO ne s'activait jamais (churn_multiplier = 1.0)

**Solution** : Transition vers MORL avec w_cost qui contrÃ´le directement les coÃ»ts de trading dans la reward.

#### ProblÃ¨me 3 : Scalarisation LinÃ©aire NaÃ¯ve (ProblÃ¨me Structurel)

**Nouvelle Analyse (Audit MORL)** : Le systÃ¨me actuel utilise une reward scalaire :

```
R = log_returns - Î»_curriculum * (churn_penalty + downside_risk) - smoothness_penalty
```

Cette **Scalarisation LinÃ©aire** crÃ©e un dilemme insoluble :
- **Î» trop faible** â†’ Overtrading (Segment 2 : 969 trades)
- **Î» trop fort** â†’ Freezing (l'agent ne trade plus)

Le coefficient Î» optimal dÃ©pend de la volatilitÃ© du marchÃ©, qui change constamment.

#### ProblÃ¨me 4 : Entropy Collapse

L'`ent_coef` descend Ã  0.015 (segment 2), crÃ©ant une politique quasi-dÃ©terministe.

---

## 2. Audit Externe : Recommandation MORL

### 2.1 Verdict de l'Auditeur (Gemini AI)

> **ğŸŸ¡ PIVOT ARCHITECTURAL REQUIS**
> 
> Le diagnostic des bugs (1 et 2) est excellent. Cependant, la stratÃ©gie de correction des pÃ©nalitÃ©s (Correction 4) repose sur une Scalarisation LinÃ©aire NaÃ¯ve. C'est une impasse connue en RL financier : trouver le Î» parfait est impossible car il dÃ©pend de la volatilitÃ© du marchÃ©.
>
> **Recommandation** : Adopter une architecture **Conditioned MORL** pour remplacer les contrÃ´leurs PID instables.

### 2.2 Principe MORL (Multi-Objective Reinforcement Learning)

Au lieu de chercher *un* coefficient unique Î», l'agent apprend une politique Ï€(a|s,w) conditionnÃ©e par un vecteur de prÃ©fÃ©rences w. L'agent apprend simultanÃ©ment :
- "Comment scalper agressivement" (w_cost â‰ˆ 0)
- "Comment investir prudemment" (w_cost â‰ˆ 1)

**Avantages** :
1. Plus de tuning infini des hyperparamÃ¨tres
2. Robustesse en production (ajuster w en temps rÃ©el sans rÃ©entraÃ®ner)
3. RÃ©solution naturelle du problÃ¨me de turnover (pÃ©nalitÃ© per-environment)

### 2.3 RÃ©Ã©valuation des Corrections sous l'Angle MORL

| Correction | Verdict MORL | Action |
|------------|--------------|--------|
| **1. Vol Scaling Mismatch** | âœ… MAINTENIR | PrÃ©-requis physique indÃ©pendant |
| **2. Turnover Calculation** | ğŸ”„ ADAPTER | Devient signal de reward secondaire |
| **3. Reward Alpha** | ğŸ›‘ REJETÃ‰ | Inutile en MORL (alpha Ã©merge naturellement) |
| **4. Coefficients Fixes** | ğŸ›‘ REMPLACÃ‰ | InjectÃ© dynamiquement via w_cost |
| **5. RÃ©duire Timesteps** | âœ… MAINTENIR | Compatible MORL |

---

## 3. Plan d'ImplÃ©mentation RÃ©visÃ©

### Phase 1 : Corrections ImmÃ©diates (Bugs)

1. **CORRECTION 1** : Aligner vol scaling train/eval
   - Fichier : `scripts/run_full_wfo.py` ligne 732
   - Changement : `max_leverage=1.0` â†’ `max_leverage=self.config.max_leverage`

2. **CORRECTION 5** : RÃ©duire timesteps Ã  30M
   - Fichier : `scripts/run_full_wfo.py` ligne 73
   - Changement : `tqc_timesteps: 90_000_000` â†’ `tqc_timesteps: 30_000_000`

**Temps estimÃ©** : 5 minutes, re-training 8-12h

### Phase 2 : ImplÃ©mentation MORL (Conditioned Network)

#### 2.1 Modification de l'Espace d'Observation

```python
# Dans batch_env.py - reset()
def reset(self):
    # ... code existant ...
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # MORL: Ã‰chantillonnage de w_cost avec distribution biaisÃ©e
    # SOTA Fix: Ã‰viter le "moyen partout" avec exploration des extrÃªmes
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    sample_type = torch.rand(self.num_envs, device=self.device)
    
    # 20% du temps: w_cost = 0 (scalping mode - profit max)
    # 20% du temps: w_cost = 1 (B&H mode - Ã©conomie max)
    # 60% du temps: Uniforme [0, 1]
    self.w_cost = torch.where(
        sample_type < 0.2,
        torch.zeros(self.num_envs, 1, device=self.device),  # Scalping
        torch.where(
            sample_type > 0.8,
            torch.ones(self.num_envs, 1, device=self.device),   # B&H
            torch.rand(self.num_envs, 1, device=self.device)    # Uniforme
        )
    )
    
    # Ajouter w_cost Ã  l'observation
    return torch.cat([obs, self.w_cost], dim=-1)
```

> **âš ï¸ RISQUE A (Audit)** : L'Ã©chantillonnage uniforme pur risque de crÃ©er un agent "moyen partout". La distribution biaisÃ©e ci-dessus force l'exploration des extrÃªmes.

#### 2.2 Modification de la Reward Function

```python
# Dans step() - get_reward()
def get_reward(self):
    # Objectif 1 : Returns (inchangÃ©)
    r_perf = torch.log1p(safe_returns) * SCALE
    
    # Objectif 2 : Costs (turnover brut, sans seuil)
    # La pÃ©nalitÃ© est locale et immÃ©diate (plus facile Ã  apprendre pour le critique)
    current_deltas = torch.abs(self.current_position - self.prev_position)
    r_cost = -current_deltas * SCALE
    
    # Reward Total : PondÃ©ration dynamique par w_cost
    # w_cost est connu de l'agent via l'observation !
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # RISQUE B (Audit): MAX_PENALTY_SCALE doit Ãªtre calibrÃ© pour que
    # r_cost * MAX_PENALTY_SCALE soit du MÃŠME ORDRE DE GRANDEUR que r_perf
    # Si log-returns â‰ˆ 0.01/step, costs doivent Ãªtre comparables
    # Surveiller TensorBoard: si reward_cost est plat Ã  0, augmenter ce facteur
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    MAX_PENALTY_SCALE = 2.0  # Ã€ calibrer selon magnitude de r_perf
    total_reward = r_perf + (self.w_cost * r_cost * MAX_PENALTY_SCALE)
    
    return total_reward
```

> **âš ï¸ RISQUE B (Audit)** : Si `r_perf >> r_cost * MAX_PENALTY_SCALE`, l'agent ignorera w_cost mÃªme Ã  w=1. Surveillez les courbes TensorBoard.

#### 2.3 Modification du RÃ©seau (input_dim + 1)

Le vecteur d'observation doit inclure w_cost :
- Ancien : `obs_dim = market_features + position + plo_levels`
- Nouveau : `obs_dim = market_features + position + plo_levels + w_cost`

#### 2.4 Suppression du Curriculum

Supprimer toute la logique de `ThreePhaseCurriculumCallback`. La randomisation de w_cost agit comme un curriculum naturel.

#### 2.5 Gestion de l'Entropie (Risque C)

```python
# Dans WFOConfig ou lors de la crÃ©ation du modÃ¨le TQC
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RISQUE C (Audit): Entropy Collapse observÃ© Ã  0.015
# MORL aide naturellement (plusieurs stratÃ©gies en tÃªte)
# Mais si collapse persiste, forcer ent_coef fixe
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ent_coef: float = 0.01  # Fixe au lieu de "auto_0.5" si collapse persiste
# OU augmenter target_entropy si utilisation de "auto"
```

> **âš ï¸ RISQUE C (Audit)** : L'entropy collapse Ã  0.015 peut persister. MORL aide mais surveillez. Si collapse, passez Ã  `ent_coef` fixe.

#### 2.6 Note Importante : Hard Reset Requis

La modification de l'espace d'observation (`input_dim + 1`) **nÃ©cessite de supprimer les anciens checkpoints** (incompatibilitÃ© de forme des tenseurs). C'est un *hard reset* du training.

**Temps estimÃ©** : 2-4h de modification

### Phase 3 : Ã‰valuation Multi-PrÃ©fÃ©rence

Au lieu d'une seule Ã©valuation, exÃ©cuter 5 passes avec w_cost âˆˆ {0.0, 0.25, 0.5, 0.75, 1.0}.

Tracer la **FrontiÃ¨re de Pareto** (Returns vs Turnover) et choisir le point opÃ©rationnel optimal.

---

## 4. Architecture MORL DÃ©taillÃ©e

### 4.1 Vecteur de RÃ©compense

```
R = [r_perf, r_cost]
  = [log1p(returns) * SCALE, -|Î”position| * SCALE]
```

Option : Ajouter un 3Ã¨me objectif pour le risque :
```
R = [r_perf, r_cost, r_risk]
  = [..., ..., -max(0, -returns)Â² * DOWNSIDE_COEF]
```

### 4.2 Scalarisation Dynamique

```
R_scalar = r_perf + w_cost * r_cost * MAX_PENALTY_SCALE
```

oÃ¹ `w_cost âˆˆ [0, 1]` est tirÃ© alÃ©atoirement Ã  chaque Ã©pisode ET inclus dans l'observation.

### 4.3 Avantages de l'Architecture MORL (ImplÃ©mentÃ©e)

| Aspect | MORL Conditioned |
|--------|------------------|
| Tuning | 1 paramÃ¨tre (MAX_PENALTY_SCALE) |
| AdaptabilitÃ© | Ajustable en temps rÃ©el via w_cost |
| Robustesse | Robuste (toutes prÃ©fÃ©rences apprises) |
| ComplexitÃ© | 1 signal w_cost dans l'observation |

---

## 5. Corrections Originales (RÃ©fÃ©rence)

### 5.1 CORRECTION 1 : Aligner Volatility Scaling Train/Eval âœ…

**Statut** : MAINTENIR (validÃ© par audit)

**Fichier** : `scripts/run_full_wfo.py`

**Avant** (ligne 732) :
```python
max_leverage=1.0,  # Disable vol scaling
```

**AprÃ¨s** :
```python
max_leverage=self.config.max_leverage,  # CohÃ©rence train/eval
```

### 5.2 CORRECTION 2 : Turnover Calculation ğŸ”„

**Statut** : ADAPTER pour MORL

**Approche Originale** : Calculer turnover moyen pour dÃ©clencher pÃ©nalitÃ© (seuil)

**Approche MORL** : Le turnover devient un **signal de rÃ©compense secondaire** sans seuil :
```python
r_cost = -|Î”position| * SCALE  # PÃ©nalitÃ© brute, pas de moyenne
```

### 5.3 CORRECTION 3 : Reward Alpha ğŸ›‘

**Statut** : REJETÃ‰ par audit

**Raison** : En MORL, l'Alpha Ã©merge naturellement si l'agent apprend Ã  gÃ©rer le risque. Pas besoin de complexifier le signal de retour.

### 5.4 CORRECTION 4 : Coefficients Fixes ğŸ›‘

**Statut** : REMPLACÃ‰ par MORL

**Raison** : Au lieu de fixer `churn_coef = 1.0`, on injecte w_cost dans l'observation. L'agent apprend toutes les valeurs possibles.

### 5.5 CORRECTION 5 : RÃ©duire Timesteps âœ…

**Statut** : MAINTENIR (validÃ© par audit)

**Fichier** : `scripts/run_full_wfo.py`

**Avant** (ligne 73) :
```python
tqc_timesteps: int = 90_000_000
```

**AprÃ¨s** :
```python
tqc_timesteps: int = 30_000_000
```

---

## 6. MÃ©triques de SuccÃ¨s Post-MORL

| MÃ©trique | Seuil Minimum | Objectif |
|----------|---------------|----------|
| FrontiÃ¨re de Pareto | Convexe et monotone | Sharpe > 1 Ã  w_cost=0.5 |
| Alpha moyen (w_cost=0.5) | > -10% | > 0% |
| Trades par segment (w_cost=0.5) | < 500 | < 200 |
| Trades par segment (w_cost=0.0) | Libre | N/A (scalping mode) |
| Trades par segment (w_cost=1.0) | < 50 | < 20 (B&H mode) |
| Entropy fin training | > 0.05 | > 0.10 |

---

## 7. Risques RÃ©siduels (SynthÃ¨se Audit Final)

| Risque | Description | Mitigation |
|--------|-------------|------------|
| **A. Ã‰chantillonnage w_cost** | Uniforme pur â†’ agent "moyen partout" | Distribution biaisÃ©e (20%/60%/20%) |
| **B. Scaling PÃ©nalitÃ©** | r_perf >> r_cost â†’ w_cost ignorÃ© | Calibrer MAX_PENALTY_SCALE, surveiller TensorBoard |
| **C. Entropy Collapse** | Politique dÃ©terministe (0.015) | MORL aide, sinon ent_coef fixe |
| **D. Hard Reset** | Checkpoints incompatibles | Supprimer anciens modÃ¨les avant Phase 2 |

---

## 8. Conclusion

L'approche MORL transforme le problÃ¨me fondamental : au lieu de **contraindre** l'agent avec des coefficients fixes (ce qui le casse), on lui **donne le choix**. L'agent apprend la relation de cause Ã  effet :

> "Si je trade trop alors que w_cost est haut, je suis puni. Si w_cost est bas, je peux scalper."

C'est la seule maniÃ¨re robuste de corriger l'overtrading en marchÃ© haussier (Segment 2) sans dÃ©truire la performance en marchÃ© baissier (Segment 0).

### Validation SOTA

Les rÃ©seaux de neurones sont d'excellents interpolateurs. En donnant w_cost en entrÃ©e, le rÃ©seau apprend une fonction continue :

```
Ï€(a|s, w) : stratÃ©gie conditionnÃ©e par prÃ©fÃ©rence
```

L'implÃ©mentation proposÃ©e (concatÃ©ner w_cost Ã  l'observation et l'utiliser pour pondÃ©rer la reward) est la mÃ©thode exacte utilisÃ©e dans les papiers de rÃ©fÃ©rence (Abels et al., 2019).

---

## 9. Next Steps RecommandÃ©s

### ImmÃ©diat (Aujourd'hui)

1. âœ… Appliquer **Correction 1** (vol scaling) - 5 minutes
2. âœ… Appliquer **Correction 5** (30M timesteps) - 2 minutes
3. ğŸ”„ Lancer un run WFO de validation avec ces 2 corrections

### Court Terme (Cette Semaine)

4. ğŸ“ ImplÃ©menter Phase 2 MORL dans `batch_env.py`
5. ğŸ§ª Tests unitaires pour vÃ©rifier dimensions tenseurs
6. ğŸ—‘ï¸ Supprimer anciens checkpoints (hard reset)

### Moyen Terme (Semaine Prochaine)

7. ğŸ“Š Run WFO complet avec architecture MORL
8. ğŸ“ˆ Tracer FrontiÃ¨re de Pareto (5 Ã©valuations avec w_cost diffÃ©rents)
9. ğŸ¯ Choisir point opÃ©rationnel optimal pour production

---

## 10. Annexes

### 10.1 Code Source Pertinent

**Reward Function** : `src/training/batch_env.py` (MORL architecture)  
**Curriculum** : `src/training/callbacks.py` (ThreePhaseCurriculumCallback)  
**Evaluation** : `scripts/run_full_wfo.py`

### 10.2 DonnÃ©es du Serveur

```
SSH: ssh -p 20941 root@158.51.110.52
RÃ©sultats: /workspace/cryptoRL/results/wfo_results.csv
Logs: /workspace/cryptoRL/logs/wfo/
```

### 10.3 Configuration Actuelle (WFOConfig)

```python
tqc_timesteps: 90_000_000  # â†’ 30_000_000
learning_rate: 1e-4
buffer_size: 2_500_000
n_envs: 1024
batch_size: 512
gamma: 0.95
ent_coef: "auto_0.5"
churn_coef: 0.5  # â†’ remplacÃ© par w_cost dynamique
smooth_coef: 1e-5  # â†’ intÃ©grÃ© dans r_cost
target_volatility: 0.05
max_leverage: 2.0
observation_noise: 0.01
critic_dropout: 0.1
```

### 10.4 RÃ©fÃ©rences MORL

- **Conditioned Network** : Abels et al., "Dynamic Weights in Multi-Objective Deep RL" (ICML 2019)
- **Pareto Front** : Van Moffaert & NowÃ©, "Multi-Objective RL using Sets of Pareto Dominating Policies" (JMLR 2014)
- **Application Finance** : Yang et al., "Safe Reinforcement Learning for Portfolio Management" (NeurIPS 2021)

### 10.5 Historique des Audits

| Date | Version | Auditeur | Action |
|------|---------|----------|--------|
| 2026-01-21 | 1.0 | Initial | Diagnostic et 5 corrections proposÃ©es |
| 2026-01-21 | 2.0 | Gemini AI | Pivot MORL recommandÃ©, Corrections 3-4 rejetÃ©es |
| 2026-01-21 | 3.0 | Claude (Validation) | ValidÃ© pour implÃ©mentation, risques documentÃ©s |
