# Audit SOTA: MORL_DESIGN.md

**Date**: 2026-01-22  
**M√©thode**: Recursive Prompt Architecture (5 audits parall√®les)  
**Document audit√©**: `docs/design/MORL_DESIGN.md` v1.0  
**Impl√©mentation**: `src/training/batch_env.py`, `src/training/callbacks.py`

---

## Table des Mati√®res

1. [R√©sum√© Ex√©cutif](#1-r√©sum√©-ex√©cutif)
2. [Audit 1: Fondations MORL (Th√©orie ML)](#2-audit-1-fondations-morl-th√©orie-ml)
3. [Audit 2: Pertinence √âconomique/Trading](#3-audit-2-pertinence-√©conomiquetrading)
4. [Audit 3: Impl√©mentation Code](#4-audit-3-impl√©mentation-code)
5. [Audit 4: Plan de Tests](#5-audit-4-plan-de-tests)
6. [Audit 5: Int√©gration Syst√®me](#6-audit-5-int√©gration-syst√®me)
7. [Synth√®se et Recommandations](#7-synth√®se-et-recommandations)

---

## 1. R√©sum√© Ex√©cutif

### Score Global: 7.8/10

| Dimension | Score | Verdict |
|-----------|-------|---------|
| Th√©orie MORL | 8.5/10 | ‚úÖ Solide, r√©f√©rences SOTA |
| √âconomie/Trading | 7.0/10 | ‚ö†Ô∏è Simplifications acceptables |
| Impl√©mentation | 8.0/10 | ‚úÖ Correcte, optimisable |
| Tests | 6.0/10 | ‚ö†Ô∏è Plan non impl√©ment√© |
| Int√©gration | 8.5/10 | ‚úÖ Coh√©rente |

### Verdict: **GO avec r√©serves mineures**

Le design MORL est **Production Ready** pour une v1.0. Les simplifications (mod√®le de co√ªts lin√©aire, scalarization lin√©aire) sont acceptables et bien document√©es. Priorit√©: impl√©menter les tests unitaires manquants.

---

## 2. Audit 1: Fondations MORL (Th√©orie ML)

### ‚úÖ Points Conformes au SOTA

1. **Architecture Conditioned Network**
   - Choix correct pour le use case (pr√©f√©rence scalaire unique)
   - Alternative Multi-Head rejet√©e √† raison (perte de partage de repr√©sentation)
   - R√©f√©rence Abels et al. (ICML 2019) pertinente et correctement appliqu√©e

2. **Scalarization Lin√©aire**
   - Appropri√©e pour un front de Pareto convexe (performance vs co√ªts)
   - Formule `R = r_perf + w √ó r_cost √ó scale` est standard
   - Propri√©t√© de convexit√© correctement identifi√©e

3. **Distribution de Sampling Biais√©e (20/60/20)**
   - Innovation pertinente vs sampling uniforme na√Øf
   - Garantit exploration des extr√™mes (w=0, w=1)
   - Coh√©rent avec curriculum sampling MORL (Yang et al., 2019)

4. **Conditions de Convergence**
   - Les 3 conditions cit√©es sont correctes:
     - ‚úÖ Sampling suffisant (distribution biais√©e)
     - ‚úÖ Capacit√© r√©seau (TQC 64√ó64 suffisant pour 1D preference)
     - ‚úÖ Exploration (gSDE + observation noise)

### ‚ö†Ô∏è √âcarts Mineurs

1. **Scalarization Non-Convexe**
   - Le design identifie correctement que la scalarization lin√©aire ne peut atteindre les points non-convexes
   - Tchebycheff mentionn√© comme alternative v2.0 ‚Üí **OK, roadmap claire**
   - *Impact*: Faible pour trading (front g√©n√©ralement convexe)

2. **Param√®tre w_cost Scalaire**
   - Un seul param√®tre de pr√©f√©rence limite l'expressivit√©
   - *Alternative SOTA*: Yang et al. proposent des vecteurs de pr√©f√©rence multi-dim
   - *Verdict*: Acceptable pour 2 objectifs, mais limitant si on ajoute d'autres objectifs (max DD, Sortino, etc.)

### ‚ùå Probl√®mes Critiques

*Aucun identifi√©.*

### üìö R√©f√©rences Manquantes

| Papier | Pourquoi Pertinent |
|--------|-------------------|
| Hayes et al. (2022) - *"A Practical Guide to MORL"* | Cit√© mais pas exploit√©: contient recommandations concr√®tes sur hyperparam√®tres MORL |
| Lu et al. (2023) - *"Pareto Set Learning for MORL"* | Alternative au conditioned network pour fronts complexes |
| Alegre et al. (2023) - *"MORL-Baselines"* | Benchmark et impl√©mentations de r√©f√©rence |

### Score Th√©orie: 8.5/10

---

## 3. Audit 2: Pertinence √âconomique/Trading

### ‚úÖ Mod√©lisation Correcte

1. **Objectif Performance (r_perf)**
   - `log1p(returns) √ó SCALE` est standard en finance quantitative
   - Log-returns additifs, bonne propri√©t√© pour RL
   - Clamp √† -0.99 √©vite log(0) ‚Üí **Correct**

2. **Interpr√©tation w_cost ‚Üí Style de Trading**
   - Mapping w=0 (scalping) ‚Üí w=1 (B&H) √©conomiquement coh√©rent
   - Continuum de styles refl√®te la r√©alit√© des traders

3. **Pareto Front Interpr√©table**
   - L'axe Sharpe vs Trades est pertinent pour un investisseur
   - Permet de choisir le profil risque/activit√© post-training

### ‚ö†Ô∏è Simplifications Acceptables

1. **Mod√®le de Co√ªts Lin√©aire**
   ```python
   r_cost = -|Œîposition| √ó SCALE
   ```
   - **Manque**: Slippage non-lin√©aire (‚àövolume), market impact, spread bid-ask variable
   - **Mitigation**: Domain randomization sur commission/slippage (impl√©ment√©!)
   - **Verdict**: Acceptable pour v1.0, le bruit couvre partiellement les non-lin√©arit√©s

2. **MAX_PENALTY_SCALE = 2.0 Fixe**
   - Calibration empirique, pas de justification formelle
   - **Recommandation**: Ajouter une phase de calibration automatique bas√©e sur les magnitudes moyennes de r_perf et r_cost sur le training set

3. **Pas de Co√ªt de Financement Overnight**
   - Le `funding_rate` existe pour shorts, mais w_cost ne le module pas
   - **Impact**: Faible pour crypto (pas de distinction jour/nuit)

### ‚ùå Erreurs de Mod√©lisation

1. **COST_PENALTY_CAP = 20.0 Asym√©trique**
   - `r_cost = clamp(r_cost, min=-20)` mais pas de max
   - **Impact**: Si r_perf explose (bug), le clamp ne prot√®ge pas
   - **Fix**: Ajouter `r_cost = clamp(r_cost, min=-20, max=0)` (co√ªts toujours n√©gatifs)

### üí° Am√©liorations Sugg√©r√©es

| Am√©lioration | Complexit√© | B√©n√©fice |
|--------------|------------|----------|
| Co√ªt de slippage ‚àövolume | Moyenne | R√©alisme accru pour gros ordres |
| w_cost per-asset (multi-asset) | Haute | Pr√©f√©rences par actif |
| Calibration auto MAX_PENALTY_SCALE | Faible | Robustesse aux changements de donn√©es |

### Score √âconomique: 7.0/10

---

## 4. Audit 3: Impl√©mentation Code

### ‚úÖ Code Correct

1. **Observation Space Dict**
   ```python
   "w_cost": spaces.Box(low=0.0, high=1.0, shape=(1,), dtype=np.float32)
   ```
   - Compatible SB3/TQC avec CombinedExtractor (implicite)
   - Shape (1,) correct pour scalar preference

2. **Sampling Distribution (reset/auto_reset)**
   ```python
   self.w_cost = torch.where(
       sample_type.unsqueeze(1) < 0.2, zeros,
       torch.where(sample_type.unsqueeze(1) > 0.8, ones, uniform)
   )
   ```
   - Logique correcte: 20% w=0, 20% w=1, 60% uniform
   - Broadcasting correct avec unsqueeze(1)

3. **Reward Calculation**
   ```python
   w_cost_squeezed = self.w_cost.squeeze(-1)  # (n_envs,)
   reward = r_perf + (w_cost_squeezed * r_cost * MAX_PENALTY_SCALE)
   ```
   - Shapes corrects: (n_envs,) partout
   - Multiplication element-wise correcte

4. **Mode √âvaluation**
   ```python
   def set_eval_w_cost(self, w_cost: Optional[float]):
       self._eval_w_cost = w_cost
   ```
   - API claire et fonctionnelle
   - Utilis√© dans reset() et _auto_reset()

### üêõ Bugs Potentiels

1. **Reproducibilit√© du Sampling w_cost**
   - `torch.rand()` dans reset() n'utilise pas le seed de l'env
   - **Conditions**: Si on veut reproduire exactement un √©pisode
   - **Fix**: Utiliser un Generator local avec seed controll√©
   ```python
   # Fix sugg√©r√©
   if self._rng is None:
       self._rng = torch.Generator(device=self.device)
   sample_type = torch.rand(self.num_envs, generator=self._rng, device=self.device)
   ```

2. **M√©moire Non-Lib√©r√©e**
   - Dans `_allocate_state_tensors()`, si appel√© plusieurs fois, les anciens tensors ne sont pas explicitement lib√©r√©s
   - **Impact**: Faible (GC Python g√®re), mais peut causer des pics m√©moire
   - **Fix**: Ajouter `del self.w_cost` avant r√©allocation

### ‚ö° Optimisations

1. **Sampling avec torch.where Nest√©s**
   - 3 allocations tenseurs (zeros, ones, uniform)
   - **Optimisation**: Pr√©-allouer dans `_allocate_state_tensors()`
   ```python
   # Dans _allocate_state_tensors:
   self._w_cost_zeros = torch.zeros(n, 1, device=device)
   self._w_cost_ones = torch.ones(n, 1, device=device)
   
   # Dans reset:
   self.w_cost = torch.where(
       sample_type.unsqueeze(1) < 0.2,
       self._w_cost_zeros,
       torch.where(sample_type.unsqueeze(1) > 0.8, self._w_cost_ones, torch.rand(...))
   )
   ```
   - **Speedup estim√©**: ~5% sur reset() (mineur car reset rare)

2. **Logging w_cost Distribution**
   - Pas de m√©trique logg√©e pour v√©rifier la distribution effective
   - **Recommandation**: Ajouter dans `get_global_metrics()`:
   ```python
   "morl/w_cost_mean": self.w_cost.mean().item(),
   "morl/w_cost_std": self.w_cost.std().item(),
   ```

### üîí S√©curit√© Num√©rique

1. **log1p Stability**
   - `safe_returns = torch.clamp(step_returns, min=-0.99)` ‚Üí log1p(‚â•0.01) ‚úÖ
   - **Robuste** aux pertes extr√™mes

2. **Division par Z√©ro**
   - Pas de division dans la reward function MORL
   - `prev_valuations` utilis√© mais jamais divis√© directement
   - ‚úÖ Pas de risque

### Score Impl√©mentation: 8.0/10

---

## 5. Audit 4: Plan de Tests

### ‚úÖ Tests Existants Valides (dans le design)

| Test | Ce qu'il couvre |
|------|-----------------|
| `test_w_cost_in_observation` | Pr√©sence et shape de w_cost |
| `test_w_cost_sampling_distribution` | Distribution 20/60/20 |
| `test_eval_w_cost_fixed` | Mode √©valuation |
| `test_reward_with_w_zero` | r_cost = 0 quand w=0 |
| `test_reward_with_w_one` | r_cost actif quand w=1 |
| `test_trained_agent_respects_w_cost` | Sensibilit√© comportementale |

### ‚ùå Cas Non Couverts (Critiques)

1. **Tests Non Impl√©ment√©s**
   - ‚ö†Ô∏è Le fichier `tests/test_morl.py` **n'existe pas**
   - Les tests sont d√©crits dans le design mais jamais cr√©√©s
   - **Risque**: R√©gression silencieuse
   - **Action**: Cr√©er `tests/test_morl.py` avec le code du design

2. **Edge Cases w_cost**
   - Pas de test pour w_cost = 0.5 (comportement interm√©diaire)
   - Pas de test pour transitions w_cost entre √©pisodes
   - **Test sugg√©r√©**:
   ```python
   def test_w_cost_changes_between_episodes():
       """w_cost should be resampled after auto-reset."""
       env = BatchCryptoEnv(n_envs=1)
       env.reset()
       w1 = env.w_cost.item()
       # Force episode end
       for _ in range(env.episode_length + 1):
           env.step_async(np.zeros((1, 1)))
           env.step_wait()
       w2 = env.w_cost.item()
       # Statistically unlikely to be equal (1/inf for continuous)
       # But we test that resampling occurred (not stuck)
   ```

3. **Robustesse NaN/Overflow**
   - Pas de test avec returns extr√™mes
   - **Test sugg√©r√©**:
   ```python
   def test_reward_stability_extreme_returns():
       """Reward should not NaN with extreme returns."""
       # Mock extreme returns
       env._calculate_rewards(
           step_returns=torch.tensor([0.99, -0.99, 10.0]),  # 10x = overflow sans clamp
           position_deltas=torch.tensor([2.0, 0.0, 1.0]),
           dones=torch.tensor([False, False, False])
       )
       assert not torch.isnan(reward).any()
   ```

### ‚ö†Ô∏è Cas Non Couverts (Secondaires)

| Cas | Priority |
|-----|----------|
| Multi-env avec w_cost h√©t√©rog√®nes | P2 |
| Interaction w_cost √ó observation_noise | P3 |
| Performance GPU du sampling | P3 |

### üìä Am√©lioration Tests Statistiques

Le test `test_w_cost_sampling_distribution` utilise:
```python
assert 0.15 < pct_zero < 0.25
```

**Probl√®me**: Intervalle ad-hoc, pas de justification statistique.

**Am√©lioration**: Utiliser un test binomial:
```python
from scipy import stats

def test_w_cost_sampling_distribution_statistical():
    n_samples = 100_000
    # ... sample w_cost ...
    
    # Test binomial pour 20% ¬± marge
    count_zero = (w == 0.0).sum()
    p_value = stats.binom_test(count_zero, n_samples, 0.2, alternative='two-sided')
    assert p_value > 0.01, f"Distribution w=0 non conforme (p={p_value:.4f})"
```

### üÜï Tests Sugg√©r√©s

```python
# tests/test_morl.py - √Ä CR√âER

import pytest
import numpy as np
import torch
from src.training.batch_env import BatchCryptoEnv


class TestMORLIntegration:
    """Integration tests for MORL with TQC."""
    
    @pytest.fixture
    def env(self, tmp_path):
        # Create minimal test data
        # ... (voir conftest.py existant)
        return BatchCryptoEnv(str(tmp_path / "test.parquet"), n_envs=4)
    
    def test_w_cost_observation_shape(self, env):
        obs = env.reset()
        assert "w_cost" in obs
        assert obs["w_cost"].shape == (4, 1)
    
    def test_w_cost_bounds(self, env):
        obs = env.reset()
        assert np.all(obs["w_cost"] >= 0.0)
        assert np.all(obs["w_cost"] <= 1.0)
    
    def test_eval_mode_fixes_w_cost(self, env):
        env.set_eval_w_cost(0.75)
        obs = env.reset()
        assert np.allclose(obs["w_cost"], 0.75)
        
        # Also fixed after step
        env.step_async(np.zeros((4, 1)))
        obs, _, _, _ = env.step_wait()
        assert np.allclose(obs["w_cost"], 0.75)
    
    def test_reward_zero_cost_when_w_zero(self, env):
        env.set_eval_w_cost(0.0)
        env.reset()
        env.step_async(np.array([[0.5]] * 4))  # Position change
        env.step_wait()
        
        # With w=0, churn component should be 0
        assert env._rew_churn.abs().max().item() < 1e-6
    
    def test_reward_nonzero_cost_when_w_one(self, env):
        env.set_eval_w_cost(1.0)
        env.reset()
        env.step_async(np.array([[0.5]] * 4))  # Position change
        env.step_wait()
        
        # With w=1, churn component should be negative (penalty)
        assert env._rew_churn.min().item() < 0
```

### Score Tests: 6.0/10

---

## 6. Audit 5: Int√©gration Syst√®me

### ‚úÖ Int√©grations Correctes

1. **MORL √ó TQC**
   - Dict observation space avec `w_cost` fonctionne via `CombinedExtractor` de SB3
   - TQC g√®re nativement les Dict spaces
   - **V√©rifi√©**: Pas besoin de feature extractor custom

2. **MORL √ó Callbacks**
   - `DetailTensorboardCallback` log `reward/pnl_component` et `reward/churn_cost`
   - M√©triques MORL accessibles via `get_global_metrics()`
   - **Logs disponibles**:
     - `internal/reward/pnl_component`
     - `internal/reward/churn_cost`
     - `internal/curriculum/lambda` (indirectement li√©)

3. **MORL √ó Domain Randomization**
   - Commission/slippage randomis√©s ind√©pendamment de w_cost
   - Pas de conflit: w_cost module la p√©nalit√©, DR module les co√ªts r√©els
   - **Synergie**: Agent robuste aux variations de co√ªts ET de pr√©f√©rences

### ‚ö†Ô∏è Frictions d'Int√©gration

1. **curriculum_lambda Non Utilis√© dans MORL**
   - Le design note: "curriculum_lambda n'est pas directement utilis√© dans la formule de r√©compense MORL"
   - `ThreePhaseCurriculumCallback` met √† jour `curriculum_lambda` mais MORL utilise `MAX_PENALTY_SCALE` fixe
   - **Impact**: Confusion potentielle, deux m√©canismes qui semblent faire la m√™me chose
   - **Mitigation**: 
     - Option A: Supprimer curriculum_lambda si MORL remplace compl√®tement
     - Option B: Utiliser `curriculum_lambda` comme multiplicateur de `MAX_PENALTY_SCALE`:
     ```python
     effective_scale = MAX_PENALTY_SCALE * self.curriculum_lambda
     reward = r_perf + (w_cost * r_cost * effective_scale)
     ```

2. **WFO √ó MORL: S√©lection de w_cost**
   - `evaluate_segment_morl()` √©value sur `[0.0, 0.5, 1.0]` (3 points)
   - **Question**: 3 points suffisent-ils pour caract√©riser le front ?
   - **Recommandation**: Utiliser 5 points `[0.0, 0.25, 0.5, 0.75, 1.0]` pour meilleure r√©solution

3. **EvalCallback √ó MORL**
   - `EvalCallbackWithNoiseControl` g√®re noise mais pas w_cost
   - **Friction**: Pas de mode "eval sur tous les w_cost" int√©gr√©
   - **Fix sugg√©r√©**: Ajouter param√®tre `eval_w_cost_values` √† `EvalCallbackWithNoiseControl`:
   ```python
   def __init__(self, eval_w_cost_values: List[float] = [0.5], ...):
       self.eval_w_cost_values = eval_w_cost_values
   
   def _on_step(self):
       for w in self.eval_w_cost_values:
           env.set_eval_w_cost(w)
           # ... evaluate ...
   ```

### ‚ùå Incompatibilit√©s

*Aucune incompatibilit√© bloquante identifi√©e.*

### üîÑ Flux de Donn√©es MORL

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                           MORL Data Flow                                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                              ‚îÇ
‚îÇ   reset() / _auto_reset()                                                    ‚îÇ
‚îÇ        ‚îÇ                                                                     ‚îÇ
‚îÇ        ‚ñº                                                                     ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ   ‚îÇ Sample w_cost ~ Biased(20% 0, 60% U[0,1], 20% 1)               ‚îÇ       ‚îÇ
‚îÇ   ‚îÇ OR use _eval_w_cost if set                                      ‚îÇ       ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
‚îÇ        ‚îÇ                                                                     ‚îÇ
‚îÇ        ‚ñº                                                                     ‚îÇ
‚îÇ   _get_observations()                                                        ‚îÇ
‚îÇ        ‚îÇ                                                                     ‚îÇ
‚îÇ        ‚ñº                                                                     ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ   ‚îÇ obs = {market, position, w_cost}                                ‚îÇ       ‚îÇ
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                              ‚îÇ       ‚îÇ
‚îÇ   ‚îÇ               Sent to TQC Policy                                ‚îÇ       ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
‚îÇ        ‚îÇ                                                                     ‚îÇ
‚îÇ        ‚ñº                                                                     ‚îÇ
‚îÇ   TQC Policy: œÄ(a|s, w_cost)                                                ‚îÇ
‚îÇ        ‚îÇ                                                                     ‚îÇ
‚îÇ        ‚ñº                                                                     ‚îÇ
‚îÇ   step_wait()                                                                ‚îÇ
‚îÇ        ‚îÇ                                                                     ‚îÇ
‚îÇ        ‚ñº                                                                     ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ   ‚îÇ _calculate_rewards(step_returns, position_deltas, dones)        ‚îÇ       ‚îÇ
‚îÇ   ‚îÇ                                                                  ‚îÇ       ‚îÇ
‚îÇ   ‚îÇ   r_perf = log1p(returns) √ó 100                                 ‚îÇ       ‚îÇ
‚îÇ   ‚îÇ   r_cost = -|Œîposition| √ó 100 (clamped)                         ‚îÇ       ‚îÇ
‚îÇ   ‚îÇ                                                                  ‚îÇ       ‚îÇ
‚îÇ   ‚îÇ   reward = r_perf + w_cost √ó r_cost √ó MAX_PENALTY_SCALE         ‚îÇ       ‚îÇ
‚îÇ   ‚îÇ            ‚ñ≤                                                     ‚îÇ       ‚îÇ
‚îÇ   ‚îÇ            ‚îî‚îÄ Conditioned on agent's preference                  ‚îÇ       ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
‚îÇ        ‚îÇ                                                                     ‚îÇ
‚îÇ        ‚ñº                                                                     ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ   ‚îÇ Logging (DetailTensorboardCallback)                             ‚îÇ       ‚îÇ
‚îÇ   ‚îÇ   - internal/reward/pnl_component                               ‚îÇ       ‚îÇ
‚îÇ   ‚îÇ   - internal/reward/churn_cost                                  ‚îÇ       ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
‚îÇ                                                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### üìà M√©triques Manquantes

| M√©trique | Utilit√© |
|----------|---------|
| `morl/w_cost_mean` | V√©rifier distribution effective |
| `morl/w_cost_std` | D√©tecter collapse vers une valeur |
| `morl/pareto_hypervolume` | Qualit√© du front (si multi-eval) |
| `morl/trades_per_w_bucket` | Sensibilit√© comportementale par bucket |

### Score Int√©gration: 8.5/10

---

## 7. Synth√®se et Recommandations

### Matrice de Risque

| Finding | Prob | Impact | Score | Owner |
|---------|------|--------|-------|-------|
| Tests MORL non impl√©ment√©s | 100% | Moyen | üî¥ HIGH | Dev |
| curriculum_lambda vs MAX_PENALTY_SCALE confus | 50% | Faible | üü° MED | Design |
| Calibration MAX_PENALTY_SCALE ad-hoc | 30% | Moyen | üü° MED | Data |
| Sampling w_cost non reproductible | 20% | Faible | üü¢ LOW | Dev |
| WFO √©value seulement 3 points Pareto | 40% | Faible | üü¢ LOW | Pipeline |

### Top 5 Actions Prioritaires

| # | Action | Effort | Impact | Deadline |
|---|--------|--------|--------|----------|
| 1 | **Cr√©er `tests/test_morl.py`** avec le code du design | 2h | ‚¨õ‚¨õ‚¨õ‚¨õ | Imm√©diat |
| 2 | Ajouter m√©triques `morl/w_cost_mean`, `morl/w_cost_std` | 30min | ‚¨õ‚¨õ‚¨õ | Sprint |
| 3 | Documenter relation curriculum_lambda / MAX_PENALTY_SCALE | 1h | ‚¨õ‚¨õ | Sprint |
| 4 | √âtendre WFO √† 5 points w_cost `[0, 0.25, 0.5, 0.75, 1]` | 1h | ‚¨õ‚¨õ | Sprint |
| 5 | Ajouter script de calibration auto MAX_PENALTY_SCALE | 4h | ‚¨õ‚¨õ‚¨õ | v2.0 |

### Verdict Final

**‚úÖ GO - Production Ready (v1.0)**

Le design MORL est solide, bien document√©, et l'impl√©mentation est correcte. Les simplifications (mod√®le de co√ªts lin√©aire, scalarization lin√©aire) sont explicitement reconnues et acceptables pour une premi√®re version.

**Conditions de release**:
1. ‚ö†Ô∏è Impl√©menter `tests/test_morl.py` avant merge en production
2. ‚ö†Ô∏è V√©rifier visuellement le Pareto front sur 1-2 segments WFO

### Roadmap v2.0

| Phase | Am√©lioration | B√©n√©fice |
|-------|--------------|----------|
| 2.1 | Scalarization Tchebycheff | Fronts non-convexes |
| 2.2 | Calibration auto MAX_PENALTY_SCALE | Robustesse cross-dataset |
| 2.3 | Multi-head policy (5 buckets) | Sp√©cialisation par style |
| 2.4 | Pareto hypervolume logging | M√©triques MORL SOTA |

---

## Annexes

### A. Checklist Pr√©-Merge

- [x] `tests/test_morl.py` cr√©√© et passant (2026-01-22)
- [x] M√©triques `morl/w_cost_mean`, `morl/w_cost_std` ajout√©es (2026-01-22)
- [x] WFO configur√© avec 5 valeurs de w_cost `EVAL_W_COST_VALUES` (2026-01-22)
- [x] Documentation curriculum_lambda clarifi√©e (section 5.4) (2026-01-22)
- [x] Fix s√©curit√©: `r_cost` clamp√© √† max=0.0 (2026-01-22)
- [ ] Pareto front visualis√© sur au moins 1 segment

### B. R√©f√©rences Suppl√©mentaires

1. **MORL-Baselines** (GitHub): https://github.com/LucasAlegre/morl-baselines
2. **Pareto Set Learning** (NeurIPS 2023): Extension pour fronts complexes
3. **SB3 Dict Observation**: https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html

---

**Auteur**: Audit automatis√© (Recursive Prompt Architecture)  
**Valid√© par**: [√Ä compl√©ter]  
**Date de revue**: 2026-01-22
