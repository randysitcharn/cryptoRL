# Audit TQC - Architecture & Plan de Diagnostic

**Date**: 2026-01-23  
**Auteur**: Analyse architecturale TQC  
**Statut**: üìã Plan d'audit propos√©

---

## üéØ Objectif

Le TQC (Truncated Quantile Critics) est le **"Chef d'Orchestre"** du syst√®me CryptoRL. S'il est mal r√©gl√©, peu importe la qualit√© du HMM ou du MAE, le syst√®me perdra de l'argent.

Ce document r√©pond aux **4 questions architecturales** critiques pour construire un outil de diagnostic parfait.

---

## üèóÔ∏è 1. L'Architecture des Entr√©es (Input Fusion)

### Question : Quelle est la strat√©gie de fusion actuelle ?

**R√©ponse : Option A (Concatenation avec Encoder MAE)**

Le TQC utilise une architecture **hi√©rarchique en deux √©tapes** :

#### √âtape 1 : Feature Extraction (FoundationFeatureExtractor)

```python
# Observation Dict re√ßue par TQC
observation = {
    "market": (batch, 64, 43),  # Fen√™tre de 64 steps √ó 43 features
    "position": (batch, 1),     # Position actuelle ‚àà [-1, 1]
    "w_cost": (batch, 1)        # Pr√©f√©rence MORL ‚àà [0, 1]
}

# Pipeline FoundationFeatureExtractor
market (64, 43) 
  ‚Üí MAE Encoder (frozen) 
  ‚Üí (64, 128) embeddings
  ‚Üí Flatten 
  ‚Üí (8192) market_features
  ‚Üí LayerNorm
  ‚Üí Concat avec position
  ‚Üí (8193) combined
  ‚Üí Linear(8193 ‚Üí 512)
  ‚Üí LayerNorm
  ‚Üí LeakyReLU
  ‚Üí (512) features
```

**R√©f√©rence** : `src/models/rl_adapter.py:FoundationFeatureExtractor`

#### √âtape 2 : Policy Network (TQCDropoutPolicy)

```python
# Les features (512D) sont ensuite pass√©es √† l'Actor/Critic
features (512)
  ‚Üí Actor MLP [256, 256] (avec dropout 0.01, LayerNorm)
  ‚Üí Action ‚àà [-1, 1]

features (512) + action
  ‚Üí Critic MLP [256, 256] (avec dropout 0.01, LayerNorm)
  ‚Üí n_quantiles=25 Q-values
```

**R√©f√©rence** : `src/models/tqc_dropout_policy.py:TQCDropoutPolicy`

### Question : Est-ce que le TQC re√ßoit les embeddings bruts du MAE ou seulement sa pr√©diction ?

**R√©ponse : ‚úÖ Embeddings bruts (recommand√©)**

Le TQC re√ßoit les **embeddings complets** du MAE encoder :
- **Input** : `market` (64, 43) - fen√™tre temporelle compl√®te
- **Output MAE** : `(64, 128)` - embeddings par timestep
- **Flatten** : `(8192)` - concat√©nation de tous les timesteps

**Avantage** : Le TQC a acc√®s √† toute l'information temporelle encod√©e, pas juste une pr√©diction ponctuelle.

### ‚ö†Ô∏è Point d'Attention : HMM Features

**Question critique** : O√π sont les features HMM (`HMM_Prob_0`, `HMM_Prob_1`, `HMM_Prob_2`, `HMM_Prob_3`, `HMM_Entropy`) ?

**R√©ponse** : Les features HMM sont **int√©gr√©es dans le `market` tensor** avant l'encodage MAE.

**Flux de donn√©es** :
```
Raw OHLCV 
  ‚Üí Feature Engineering (FFD, Volatility, etc.)
  ‚Üí HMM Regime Detection
  ‚Üí Ajout colonnes HMM_Prob_* et HMM_Entropy
  ‚Üí RobustScaler (fit sur train uniquement)
  ‚Üí market tensor (64, 43) incluant HMM features
  ‚Üí MAE Encoder
  ‚Üí TQC
```

**R√©f√©rence** : `src/data_engineering/manager.py:RegimeDetector.get_belief_states_df()`

**V√©rification n√©cessaire** : S'assurer que les features HMM sont bien pr√©sentes dans le `market` tensor et que le MAE les encode correctement.

---

## üß† 2. La Nature du "Cerveau" (Core Policy)

### Question : Quel type de mod√®le est le TQC ?

**R√©ponse : Reinforcement Learning (TQC - Truncated Quantile Critics)**

Le TQC est un algorithme **Actor-Critic off-policy** qui :

1. **N'apprend PAS** √† pr√©dire une erreur du MAE
2. **N'est PAS** un meta-learner supervis√©
3. **Apprend une politique** `œÄ(a|s, w_cost)` pour maximiser la r√©compense MORL

#### Architecture TQC

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    TQC (SB3)                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
‚îÇ  ‚îÇ Actor (œÄ)     ‚îÇ         ‚îÇ Critics (Q)   ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ               ‚îÇ         ‚îÇ              ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ Input:        ‚îÇ         ‚îÇ Input:       ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ  features(512)‚îÇ         ‚îÇ  features(512)‚îÇ          ‚îÇ
‚îÇ  ‚îÇ               ‚îÇ         ‚îÇ  + action    ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ MLP:          ‚îÇ         ‚îÇ              ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ  [256, 256]   ‚îÇ         ‚îÇ MLP:         ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ  + Dropout    ‚îÇ         ‚îÇ  [256, 256]  ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ  + LayerNorm  ‚îÇ         ‚îÇ  + Dropout    ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ               ‚îÇ         ‚îÇ  + LayerNorm  ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ Output:       ‚îÇ         ‚îÇ              ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ  action ‚àà [-1,1]‚îÇ        ‚îÇ Output:      ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ               ‚îÇ         ‚îÇ  n_quantiles=25‚îÇ          ‚îÇ
‚îÇ  ‚îÇ               ‚îÇ         ‚îÇ  Q-values     ‚îÇ            ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  Loss: Actor-Critic avec Truncated Quantiles          ‚îÇ
‚îÇ  - Critic Loss: Huber loss sur quantiles              ‚îÇ
‚îÇ  - Actor Loss: -Q(s, œÄ(s)) (policy gradient)         ‚îÇ
‚îÇ  - Entropy Bonus: Exploration (gSDE)                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**R√©f√©rence** : `docs/design/MODELES_RL_DESIGN.md:3.1 TQC`

#### Algorithme d'Apprentissage

```python
# Boucle d'entra√Ænement TQC
for step in range(total_timesteps):
    # 1. Sample action from policy
    action = œÄ(observation) + gSDE_noise
    
    # 2. Execute in environment
    next_obs, reward, done = env.step(action)
    
    # 3. Store transition
    replay_buffer.add(obs, action, reward, next_obs, done)
    
    # 4. Update TQC (off-policy)
    if step > learning_starts:
        batch = replay_buffer.sample(batch_size=2048)
        
        # Critic update: Estimate Q-distribution (25 quantiles)
        critic_loss = huber_loss(quantiles, target_quantiles)
        
        # Actor update: Maximize Q(s, œÄ(s))
        actor_loss = -Q(s, œÄ(s)) + entropy_bonus
        
        # Soft update target networks (œÑ=0.005)
        Œ∏_target = œÑ * Œ∏ + (1-œÑ) * Œ∏_target
```

**R√©f√©rence** : `src/training/train_agent.py:train()`

---

## üéØ 3. La Sortie D√©cisionnelle (Action Space)

### Question : Que contr√¥le le TQC exactement ?

**R√©ponse : Action directe (Position cible)**

Le TQC contr√¥le directement l'**action** (position cible) :

```python
action ‚àà [-1, 1]  # -1 = 100% short, 0 = cash, +1 = 100% long
```

#### Types de contr√¥le possibles :

| Type | Description | Impl√©ment√© ? |
|------|-------------|--------------|
| **Signal de Confiance** | Multiplicateur de taille | ‚ùå Non |
| **Signal "Go / No-Go"** | Filtre binaire (Trade/Cash) | ‚ùå Non |
| **Side Correction** | Peut inverser le MAE | ‚úÖ **OUI** |
| **Action Directe** | Position cible ‚àà [-1, 1] | ‚úÖ **OUI** |

#### Architecture de D√©cision

```
TQC Policy œÄ(s, w_cost)
  ‚Üì
Action ‚àà [-1, 1]
  ‚Üì
Action Discretization (optionnel, 21 niveaux si action_discretization=0.1)
  ‚Üì
New Position = discretize(action)
  ‚Üì
Execute Trade: Œîposition = new_position - current_position
  ‚Üì
Reward = r_perf + w_cost * r_cost * MAX_PENALTY_SCALE
```

**R√©f√©rence** : `src/training/batch_env.py:BatchCryptoEnv._calculate_reward()`

#### Peut-il inverser le MAE ?

**OUI**, le TQC peut compl√®tement ignorer ou inverser toute suggestion du MAE car :

1. Le MAE est **frozen** (pas de gradient)
2. Le TQC apprend **end-to-end** la politique optimale
3. Si le MAE sugg√®re "Long" mais le TQC voit un pi√®ge (via HMM entropy √©lev√©e, volatilit√©, etc.), il peut shorter

**Exemple** :
```python
# MAE encodeur sugg√®re pattern bullish (via embeddings)
# Mais TQC voit :
#   - HMM_Entropy √©lev√©e (r√©gime incertain)
#   - Position d√©j√† longue (surcharge)
#   - w_cost √©lev√© (co√ªts importants)
# ‚Üí TQC peut d√©cider action = -0.5 (short partiel)
```

**R√©f√©rence** : `src/models/rl_adapter.py:FoundationFeatureExtractor` (encoder frozen)

---

## üìâ 4. La Loss Function (Si Supervis√©)

### Question : Si le TQC est entra√Æn√©, qu'est-ce qu'il optimise ?

**R√©ponse : Reinforcement Learning (pas de loss supervis√©e)**

Le TQC est entra√Æn√© via **Reinforcement Learning**, donc il optimise :

#### Objectif : Maximiser la r√©compense cumul√©e (discount√©e)

```python
# Objectif RL
J(œÄ) = E[Œ£ Œ≥^t * r_t]

# O√π la reward est MORL (Multi-Objective)
r_t = r_perf + w_cost * r_cost * MAX_PENALTY_SCALE

# Avec :
r_perf = log1p(clamp(returns, -0.2, 0.2)) * SCALE  # SCALE=100
r_cost = -|Œîposition| * SCALE
w_cost ‚àà [0, 1]  # Pr√©f√©rence MORL (dans observation)
MAX_PENALTY_SCALE = 2.0
```

**R√©f√©rence** : `src/training/batch_env.py:BatchCryptoEnv._calculate_reward()`

#### Loss Functions TQC

**Critic Loss** (Huber loss sur quantiles) :
```python
# Pour chaque critic (n_critics=2)
# Pour chaque quantile (n_quantiles=25)
quantile_loss = huber_loss(
    predicted_quantile,
    target_quantile  # Calcul√© via Bellman backup
)

# Truncation : Drop top_quantiles_to_drop=2 quantiles extr√™mes
critic_loss = mean(quantile_losses[keep_quantiles])
```

**Actor Loss** (Policy gradient) :
```python
# Maximize Q(s, œÄ(s))
actor_loss = -mean(Q(s, œÄ(s))) + entropy_bonus

# O√π Q(s, a) = mean(truncated_quantiles)
```

**R√©f√©rence** : `sb3_contrib.tqc.policies.TQCPolicy` (impl√©mentation SB3)

#### Ce que le TQC n'optimise PAS

| M√©trique | Optimis√© ? | Raison |
|----------|-----------|--------|
| **Accuracy du trade** | ‚ùå Non | Pas de labels supervis√©s |
| **PnL pond√©r√©** | ‚úÖ Indirectement | Via reward r_perf |
| **Calibration (BCE)** | ‚ùå Non | Pas de classification |
| **Sharpe Ratio** | ‚úÖ Indirectement | Via reward (log-returns) |

**Note** : Le TQC optimise le **PnL via la reward**, mais pas directement. La reward est une approximation du PnL (log-returns) avec p√©nalit√©s de co√ªts.

---

## üìã Plan de Diagnostic TQC Propos√©

### A. Analyse de la "M√©ta-Confiance" (Calibration Audit)

**Objectif** : V√©rifier si le TQC sait quand il a raison.

#### A.1 Reliability Diagram

**Test** : Quand le TQC est "confiant" (Q-values √©lev√©es), est-ce qu'on gagne vraiment ?

```python
# M√©thodologie
1. Collecter (Q_value, actual_return) pour chaque step
2. Binner Q_values en d√©ciles [0-10%, 10-20%, ..., 90-100%]
3. Pour chaque bin, calculer :
   - Mean Q_value (confiance pr√©dite)
   - Mean actual_return (r√©alit√©)
   - Win rate (% de trades profitables)
4. Plot Reliability Diagram:
   - Axe X: Q_value bins
   - Axe Y: Actual return / Win rate
   - Ligne id√©ale: y = x (calibration parfaite)
```

**M√©triques** :
- **ECE (Expected Calibration Error)** : √âcart moyen entre confiance et r√©alit√©
- **Brier Score** : Score de calibration (0 = parfait)
- **Overconfidence** : Si Q_values > actual_returns syst√©matiquement

#### A.2 Entropy Correlation

**Test** : Le TQC baisse-t-il sa confiance quand `HMM_Entropy` est √©lev√©e ?

```python
# M√©thodologie
1. Collecter (HMM_Entropy, Q_value_std, action_magnitude) pour chaque step
2. Calculer corr√©lations :
   - corr(HMM_Entropy, Q_value_std)  # Incertitude TQC vs incertitude HMM
   - corr(HMM_Entropy, |action|)     # Position size vs incertitude HMM
3. Plot scatter:
   - Axe X: HMM_Entropy
   - Axe Y: Q_value_std (ou |action|)
   - Attendu: Corr√©lation n√©gative (entropy √©lev√©e ‚Üí confiance faible)
```

**V√©rification** : Si corr√©lation faible/nulle, le TQC ignore le HMM (probl√®me d'architecture).

---

### B. Analyse d'Attribution (Feature Importance)

**Objectif** : Comprendre pourquoi le TQC prend une d√©cision.

#### B.1 SHAP Values / Gradient Attribution

**Test** : Quelles features influencent le plus les d√©cisions TQC ?

```python
# M√©thodologie
1. Sample N observations (N=1000)
2. Pour chaque observation:
   - Calculer gradients: ‚àá_features Q(s, a)
   - Ou utiliser SHAP: shap_values = explainer.shap_values(obs)
3. Agr√©gation:
   - Mean |gradient| par feature
   - Feature importance ranking
4. V√©rifications sp√©cifiques:
   - HMM_Prob_* : Impact sur action ?
   - HMM_Entropy : Impact sur |action| ?
   - Position : Impact sur action (hold vs trade) ?
   - w_cost : Impact sur action (scalping vs B&H) ?
```

**M√©triques** :
- **Feature Importance Ranking** : Top 10 features les plus influentes
- **HMM Sensitivity** : Si HMM features sont en bas du ranking ‚Üí probl√®me
- **Position Sensitivity** : Si position n'influence pas ‚Üí probl√®me (agent ignore son √©tat)

#### B.2 Ablation Study

**Test** : Que se passe-t-il si on retire certaines features ?

```python
# M√©thodologie
1. Baseline: TQC avec toutes les features
2. Ablations:
   - Sans HMM features (Prob_*, Entropy)
   - Sans position
   - Sans w_cost
   - Sans MAE embeddings (features brutes uniquement)
3. Comparer:
   - Sharpe Ratio
   - Win Rate
   - Max Drawdown
   - Action distribution
```

**V√©rification** : Si retirer HMM ne change rien ‚Üí TQC ignore HMM (probl√®me).

---

### C. Analyse de la "Value Add" (PnL Uplift)

**Objectif** : Test financier ultime - le TQC ajoute-t-il de la valeur ?

#### C.1 Baseline Comparison

**Test** : Comparer PnL avec/sans TQC

```python
# M√©thodologie
1. Courbe A (Naive) : Suivre aveugl√©ment le MAE
   - action = sign(MAE_prediction) * 1.0  # Full size
   
2. Courbe B (TQC) : Utiliser la politique TQC
   - action = œÄ(observation)
   
3. Courbe C (Oracle) : Action parfaite (look-ahead)
   - action = sign(future_return) * 1.0
   
4. M√©triques comparatives:
   - Sharpe Ratio
   - Total Return
   - Max Drawdown
   - Win Rate
   - Calmar Ratio
```

**M√©trique cl√©** : **Delta = Sharpe_TQC - Sharpe_Naive**

Si Delta < 0 ‚Üí TQC d√©truit de la valeur (probl√®me critique).

#### C.2 Regime-Specific Performance

**Test** : Le TQC performe-t-il mieux dans certains r√©gimes HMM ?

```python
# M√©thodologie
1. Segmenter les trades par HMM state dominant
   - State 0 (Crash): Trades o√π HMM_Prob_0 > 0.5
   - State 1 (Downtrend): Trades o√π HMM_Prob_1 > 0.5
   - State 2 (Range): Trades o√π HMM_Prob_2 > 0.5
   - State 3 (Uptrend): Trades o√π HMM_Prob_3 > 0.5

2. Calculer m√©triques par r√©gime:
   - Sharpe Ratio par r√©gime
   - Win Rate par r√©gime
   - Avg Return par r√©gime

3. V√©rification:
   - TQC devrait outperformer dans r√©gimes incertains (entropy √©lev√©e)
   - TQC devrait √™tre conservateur en Crash (State 0)
```

**V√©rification** : Si TQC performe mal dans certains r√©gimes ‚Üí probl√®me de calibration.

---

### D. Analyse de la Distribution des Actions

**Objectif** : V√©rifier que le TQC explore correctement l'espace d'action.

#### D.1 Action Distribution Analysis

```python
# M√©thodologie
1. Collecter toutes les actions pr√©dites par TQC
2. Analyser:
   - Histogramme des actions
   - Saturation: % d'actions √† ¬±1.0 (ou proche)
   - Mode: Distribution unimodale vs multimodale
   - Entropy des actions: H(œÄ) = -Œ£ œÄ(a) log œÄ(a)

3. V√©rifications:
   - Si > 95% actions satur√©es ‚Üí Policy collapse
   - Si distribution trop √©troite ‚Üí Pas d'exploration
   - Si entropy trop faible ‚Üí Policy trop d√©terministe
```

**R√©f√©rence** : `docs/audit/AUDIT_SMALL_POSITIONS.md` (d√©j√† identifi√©)

---

### E. Analyse de la Convergence

**Objectif** : V√©rifier que le TQC apprend correctement.

#### E.1 Training Curves Analysis

```python
# M√©triques √† monitorer (d√©j√† dans TensorBoard)
1. Q-values:
   - Mean Q-value (devrait augmenter)
   - Std Q-value (devrait diminuer si convergence)
   
2. Actor/Critic Loss:
   - Critic loss (devrait diminuer)
   - Actor loss (devrait converger)
   
3. Entropy:
   - Policy entropy (devrait diminuer progressivement)
   
4. Rewards:
   - Mean reward (devrait augmenter)
   - Reward variance (devrait diminuer)
```

**V√©rification** : Si Q-values divergent ou loss explose ‚Üí probl√®me de stabilit√©.

---

### F. Analyse des Quantiles (Risk Awareness)

**Objectif** : V√©rifier que le TQC est "conscient du risque" gr√¢ce √† ses quantiles.

#### F.1 Inter-Quantile Range (IQR) vs HMM_Entropy

**Test** : Quand `HMM_Entropy` est √©lev√©e (incertitude march√©), l'IQR du TQC doit augmenter (incertitude mod√®le).

**M√©thodologie** :
```python
# Pour chaque step:
1. Extraire les 25 quantiles du critic TQC
   - quantiles = model.policy.critic(obs, action)
   - Shape: (batch, n_critics=2, n_quantiles=25)
   - Moyenne sur les critics: (batch, 25)

2. Calculer IQR = Q90 - Q10
   - q10_idx = int(0.10 * 25) = 2
   - q90_idx = int(0.90 * 25) = 22
   - iqr = quantiles[q90_idx] - quantiles[q10_idx]

3. Extraire HMM_Entropy depuis le dataframe de test

4. Calculer corr√©lation: corr(HMM_Entropy, IQR)
   - Attendu: Corr√©lation positive (high entropy ‚Üí high IQR)
   - Si corr√©lation < 0.3 ‚Üí TQC ignore l'incertitude du march√© (OVERCONFIDENCE)
```

**M√©triques** :
- **Corr√©lation (HMM_Entropy, IQR)** : Doit √™tre ‚â• 0.3
- **Overconfidence Flag** : Si corr√©lation < 0.3, TQC est trop confiant

**Interpr√©tation** :
- ‚úÖ **Corr√©lation ‚â• 0.3** : TQC r√©pond correctement √† l'incertitude du march√©
- ‚ö†Ô∏è **Corr√©lation < 0.3** : TQC est overconfident (dangerous) - ignore l'incertitude HMM

**Plot** : Scatter plot `HMM_Entropy` (axe X) vs `IQR` (axe Y) avec ligne de corr√©lation.

**R√©f√©rence** : `scripts/audit_pipeline.py:analyze_tqc_quantiles()`

---

## üöÄ Prochaines √âtapes

### 1. Impl√©mentation du Script d'Audit

**Fichier** : `scripts/audit_tqc.py`

**Fonctions √† impl√©menter** :
- `analyze_tqc_calibration(model, test_data)` ‚Üí Reliability Diagram
- `analyze_tqc_attribution(model, test_data)` ‚Üí SHAP/Gradient Attribution
- `analyze_tqc_value_add(model, test_data, baseline)` ‚Üí PnL Uplift
- `analyze_tqc_action_distribution(model, test_data)` ‚Üí Action Analysis
- `analyze_tqc_regime_performance(model, test_data)` ‚Üí Regime-Specific

### 2. Collecte de Donn√©es

**Pr√©requis** :
- Mod√®le TQC entra√Æn√© (`weights/wfo/segment_X/tqc.zip`)
- Donn√©es de test (parquet avec HMM features)
- Baseline MAE (pour comparaison)

### 3. G√©n√©ration de Rapports

**Output** :
- `results/audit_tqc/calibration_report.md`
- `results/audit_tqc/attribution_report.md`
- `results/audit_tqc/value_add_report.md`
- `results/audit_tqc/plots/*.png`

---

## üìö R√©f√©rences

- **TQC Paper** : Kuznetsov et al. (2020) - "Controlling Overestimation Bias with Truncated Mixture of Continuous Distributional Quantile Critics"
- **DroQ Paper** : Hiraoka et al. (2021) - "Dropout Q-Functions for Doubly Efficient RL"
- **MORL Paper** : Abels et al. (2019) - "Multi-Objective Reinforcement Learning"
- **Design Docs** :
  - `docs/design/MODELES_RL_DESIGN.md`
  - `docs/design/MORL_DESIGN.md`
  - `docs/design/DROPOUT_TQC_DESIGN.md`
- **Code Sources** :
  - `src/models/rl_adapter.py` - FoundationFeatureExtractor
  - `src/models/tqc_dropout_policy.py` - TQCDropoutPolicy
  - `src/training/train_agent.py` - Training loop
  - `src/training/batch_env.py` - Environment & Reward

---

**Statut** : ‚úÖ Architecture cartographi√©e - Pr√™t pour impl√©mentation audit
