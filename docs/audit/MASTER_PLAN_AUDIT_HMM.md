# Master Plan: Audit HMM (Hidden Markov Model) - CryptoRL

**Date**: 2026-01-22  
**M√©thode**: State-of-the-Art Audit Framework  
**Objectif**: Audit exhaustif et critique du syst√®me HMM pour d√©tection de r√©gimes de march√©  
**R√©f√©rence**: Hamilton (1989), Rabiner (1989), fHMM (R), Shu et al. (2024)

---

## üìã M√©ta-Informations

- **Complexit√© totale estim√©e**: 42 points
- **Nombre de prompts atomiques**: 15
- **Chemins parall√©lisables**: 
  - Batch 1: P1.1 ‚Äñ P1.2 ‚Äñ P1.3 ‚Äñ P1.4
  - Batch 2: P2.1 ‚Äñ P2.2 ‚Äñ P2.3 ‚Äñ P2.4
  - Batch 3: P3.1 ‚Äñ P3.2 ‚Äñ P3.3
  - Batch 4: P4.1 ‚Äñ P4.2
  - Batch 5: P5

---

## üéØ Phase 0 : Clarification (Pr√©-Analyse)

| Question | R√©ponse | Statut |
|----------|---------|--------|
| L'objectif final est-il mesurable/v√©rifiable ? | Rapport d'audit avec scores par composant, findings critiques (P0/P1/P2), recommandations prioris√©es, m√©triques quantitatives | ‚úÖ |
| Les contraintes techniques sont-elles explicites ? | Python 3.10+, hmmlearn, sklearn, donn√©es horaires BTC, WFO compatible | ‚úÖ |
| Le scope est-il born√© ? | HMM uniquement (RegimeDetector) - features, training, prediction, alignment | ‚úÖ |

**Scope IN**:
- `RegimeDetector` class (`src/data_engineering/manager.py`)
- HMM features engineering (HMM_Trend, HMM_Vol, HMM_Momentum, HMM_RiskOnOff, HMM_VolRatio)
- GMM-HMM configuration (n_components=4, n_mix=2, transition_penalty)
- K-Means warm start initialization
- Archetype Alignment (Hungarian Algorithm)
- Quality validation logic
- WFO integration (fit on train, predict on test)
- TensorBoard logging

**Scope OUT**:
- Feature engineering g√©n√©ral (FFD, Z-Score) ‚Üí d√©j√† audit√©
- Data pipeline orchestration ‚Üí d√©j√† audit√©
- RL agent utilisant Prob_* features ‚Üí scope mod√®les RL

---

## üå≥ Arbre de D√©composition

```
Root: "Audit HMM SOTA"
‚îÇ
‚îú‚îÄ‚Üí P1: Audit Architecture & Design (parall√®le)
‚îÇ   ‚îú‚îÄ‚Äñ P1.1: Audit Features Engineering (ATOMIC)
‚îÇ   ‚îú‚îÄ‚Äñ P1.2: Audit GMM-HMM Configuration (ATOMIC)
‚îÇ   ‚îú‚îÄ‚Äñ P1.3: Audit Initialization Strategy (ATOMIC)
‚îÇ   ‚îî‚îÄ‚Äñ P1.4: Audit Archetype Alignment (ATOMIC)
‚îÇ
‚îú‚îÄ‚Üí P2: Audit Validation & Quality (parall√®le, d√©pend P1)
‚îÇ   ‚îú‚îÄ‚Äñ P2.1: Audit Quality Metrics (ATOMIC)
‚îÇ   ‚îú‚îÄ‚Äñ P2.2: Audit Model Selection (ATOMIC)
‚îÇ   ‚îú‚îÄ‚Äñ P2.3: Audit Convergence & Stability (ATOMIC)
‚îÇ   ‚îî‚îÄ‚Äñ P2.4: Audit Retry Logic (ATOMIC)
‚îÇ
‚îú‚îÄ‚Üí P3: Audit Data Leakage & WFO (parall√®le, d√©pend P2)
‚îÇ   ‚îú‚îÄ‚Äñ P3.1: Audit WFO Integration (ATOMIC)
‚îÇ   ‚îú‚îÄ‚Äñ P3.2: Audit Context Buffer (ATOMIC)
‚îÇ   ‚îî‚îÄ‚Äñ P3.3: Audit Feature Lookback Windows (ATOMIC)
‚îÇ
‚îú‚îÄ‚Üí P4: Audit Robustness & Reproducibility (parall√®le, d√©pend P3)
‚îÇ   ‚îú‚îÄ‚Äñ P4.1: Audit Reproducibility (ATOMIC)
‚îÇ   ‚îî‚îÄ‚Äñ P4.2: Audit Edge Cases & Numerical Stability (ATOMIC)
‚îÇ
‚îî‚îÄ‚Üí P5: Synth√®se & Recommandations (ATOMIC, d√©pend P4)
```

**L√©gende**: ‚Üí s√©quentiel | ‚Äñ parall√®le

---

## üìù Prompts Ex√©cutables

---

### Batch 1 : Audit Architecture & Design

---

### √âtape 1.1 : Audit Features Engineering

**ID**: `P1.1`  
**D√©pendances**: Aucune  
**Parall√©lisable avec**: P1.2, P1.3, P1.4  
**Score complexit√©**: 7 (domaine sp√©cialis√© + validation th√©orique)

**Prompt Optimis√©**:
```text
## Audit HMM Features Engineering

### Persona
Tu es un quant researcher senior avec 10+ ans d'exp√©rience en mod√©lisation de r√©gimes de march√©. Tu as publi√© sur HMM pour time series financi√®res et connais les pi√®ges classiques (look-ahead bias, stationnarit√©, multicollin√©arit√©).

### Contexte
- Projet: CryptoRL - Trading RL pour cryptomonnaies
- HMM Features: HMM_Trend, HMM_Vol, HMM_Momentum, HMM_RiskOnOff, HMM_VolRatio
- Fichier: `src/data_engineering/manager.py` (lignes 122-194)
- Window: 168h (7 jours) pour smoothing
- Data: BTC hourly, multi-asset (SPX, DXY)

### T√¢che
Auditer la qualit√© et la validit√© des features HMM selon les standards SOTA:

1. **Stationnarit√© & Look-Ahead Bias**
   - V√©rifier que les rolling windows n'utilisent pas de donn√©es futures
   - Valider que les features sont stationnaires (ADF test, KPSS test)
   - V√©rifier l'absence de leakage temporel

2. **Feature Engineering Quality**
   - HMM_Trend: MA(LogRet, 168h) - valider la pertinence de la fen√™tre
   - HMM_Vol: MA(Parkinson, 168h) - v√©rifier la coh√©rence avec la volatilit√© r√©elle
   - HMM_Momentum: RSI(14) / 100 - valider la normalisation et les bornes
   - HMM_RiskOnOff: MA(SPX_ret - DXY_ret, 168h) - v√©rifier la corr√©lation avec BTC
   - HMM_VolRatio: Vol(24h) / Vol(168h) - valider le ratio comme early warning

3. **Clipping & Numerical Stability**
   - V√©rifier que les clips sont justifi√©s th√©oriquement
   - Valider que les bornes ne tronquent pas trop de signal
   - Tester la stabilit√© num√©rique (NaN, Inf)

4. **Multicollin√©arit√©**
   - Calculer la matrice de corr√©lation entre features
   - Identifier les features redondantes (VIF > 5)
   - Recommander des features alternatives si n√©cessaire

5. **Feature Selection vs Domain Knowledge**
   - Comparer avec la litt√©rature (Hamilton 1989, Ang & Timmermann 2002)
   - Valider que HMM_Funding a bien √©t√© retir√© (audit P1.2)
   - Proposer des features additionnelles SOTA si pertinentes

### Livrables
1. Rapport d'audit avec scores par feature (0-10)
2. Tests de stationnarit√© (ADF, KPSS) avec r√©sultats
3. Matrice de corr√©lation et VIF
4. Liste de findings (P0/P1/P2) avec recommandations
5. Code de validation reproductible
```

**M√©triques de Succ√®s**:
- ‚úÖ Toutes les features sont stationnaires (p-value < 0.05 ADF)
- ‚úÖ Aucun look-ahead bias d√©tect√©
- ‚úÖ VIF < 5 pour toutes les features
- ‚úÖ Clipping justifi√© th√©oriquement
- ‚úÖ Features align√©es avec la litt√©rature SOTA

---

### √âtape 1.2 : Audit GMM-HMM Configuration

**ID**: `P1.2`  
**D√©pendances**: Aucune  
**Parall√©lisable avec**: P1.1, P1.3, P1.4  
**Score complexit√©**: 8 (th√©orie HMM + hyperparam√®tres)

**Prompt Optimis√©**:
```text
## Audit GMM-HMM Configuration

### Persona
Tu es un chercheur en statistique computationnelle avec expertise en HMM et GMM. Tu connais les pi√®ges classiques (overfitting, underfitting, identifiability, label switching).

### Contexte
- Mod√®le: GMMHMM (hmmlearn)
- Configuration actuelle:
  - n_components: 4 (Crash, Downtrend, Range, Uptrend)
  - n_mix: 2 (mixture components)
  - covariance_type: 'diag'
  - n_iter: 200
  - min_covar: 1e-3
  - transition_penalty: 0.1 (Sticky HMM)
- Fichier: `src/data_engineering/manager.py` (lignes 504-512, 262-297)

### T√¢che
Auditer la configuration GMM-HMM selon les standards SOTA:

1. **Model Selection (n_components, n_mix)**
   - Valider que n_components=4 est optimal (AIC, BIC, ICL)
   - Tester n_components ‚àà {3, 4, 5, 6} avec information criteria
   - Valider que n_mix=2 est suffisant (pas d'overfitting)
   - Comparer avec la litt√©rature (4 r√©gimes = standard en finance)

2. **Covariance Structure**
   - Valider 'diag' vs 'full' vs 'tied' vs 'spherical'
   - V√©rifier la stabilit√© num√©rique (condition number)
   - Tester si 'full' am√©liore la qualit√© sans overfitting

3. **EM Algorithm Configuration**
   - Valider n_iter=200 (convergence garantie?)
   - V√©rifier le tolerance (monitor_.tol)
   - Analyser l'historique de convergence (log-likelihood)
   - D√©tecter les cas de non-convergence

4. **Regularization (min_covar)**
   - Valider min_covar=1e-3 (pas trop restrictif?)
   - Tester min_covar ‚àà {1e-4, 1e-3, 1e-2}
   - V√©rifier que la r√©gularisation n'√©crase pas le signal

5. **Sticky HMM (Transition Penalty)**
   - Valider la formule: A_sticky = A √ó (1-p) + I √ó p
   - V√©rifier que penalty=0.1 est optimal (test grid search)
   - Analyser l'impact sur la dur√©e moyenne des r√©gimes
   - Comparer avec la litt√©rature (Shu et al. 2024)

6. **Identifiability & Label Switching**
   - V√©rifier que le mod√®le est identifiable (pas de modes d√©g√©n√©r√©s)
   - Valider que l'Archetype Alignment r√©sout le label switching
   - Tester la stabilit√© du mapping entre runs

### Livrables
1. Rapport d'audit avec scores par hyperparam√®tre
2. Courbes AIC/BIC/ICL pour diff√©rents n_components
3. Analyse de convergence (log-likelihood history)
4. Grid search results pour transition_penalty
5. Liste de findings (P0/P1/P2) avec recommandations
6. Code de validation reproductible
```

**M√©triques de Succ√®s**:
- ‚úÖ n_components=4 optimal selon AIC/BIC
- ‚úÖ Convergence EM garantie (monitor_.converged > 95%)
- ‚úÖ min_covar optimal (pas de singularit√©, pas d'over-regularization)
- ‚úÖ transition_penalty=0.1 optimal (grid search)
- ‚úÖ Mod√®le identifiable (pas de modes d√©g√©n√©r√©s)

---

### √âtape 1.3 : Audit Initialization Strategy

**ID**: `P1.3`  
**D√©pendances**: Aucune  
**Parall√©lisable avec**: P1.1, P1.2, P1.4  
**Score complexit√©**: 6 (initialization heuristics)

**Prompt Optimis√©**:
```text
## Audit HMM Initialization Strategy

### Persona
Tu es un expert en optimisation non-convexe avec expertise en initialization heuristics pour mod√®les g√©n√©ratifs (HMM, GMM, VAE).

### Contexte
- Strat√©gie: K-Means warm start
- Fichier: `src/data_engineering/manager.py` (lignes 196-245, 489-521)
- Flow: K-Means ‚Üí inject centers ‚Üí add noise ‚Üí fit HMM
- Random state: self.random_state + attempt * 17 (retry logic)

### T√¢che
Auditer la strat√©gie d'initialisation selon les standards SOTA:

1. **K-Means Warm Start**
   - Valider que K-Means am√©liore la convergence vs random init
   - V√©rifier que n_init=10 est suffisant pour K-Means
   - Tester si K-Means++ est meilleur que K-Means standard
   - Analyser l'inertia K-Means (qualit√© des clusters)

2. **Noise Injection**
   - Valider que noise ~ N(0, 0.1) est optimal
   - V√©rifier que le noise diff√©rencie bien les mixture components
   - Tester diff√©rents niveaux de noise (0.05, 0.1, 0.2)
   - Analyser l'impact sur la convergence

3. **Reproducibility Issues**
   - Identifier les sources de non-reproductibilit√©:
     - K-Means random_state changeant entre retries
     - HMM random_state changeant entre retries
     - Noise injection avec np.random.seed()
   - Proposer une solution d√©terministe

4. **Alternative Initialization Methods**
   - Comparer avec d'autres m√©thodes SOTA:
     - Spectral initialization
     - Moment matching
     - Variational Bayes initialization
   - √âvaluer si une m√©thode alternative est meilleure

5. **Retry Logic Impact**
   - Analyser si les retries am√©liorent vraiment la qualit√©
   - V√©rifier que le best selection (max active states) est optimal
   - Proposer une m√©trique de qualit√© secondaire (separation_score)

### Livrables
1. Rapport d'audit avec comparaison init methods
2. Tests de reproductibilit√© (m√™me seed ‚Üí m√™me r√©sultat)
3. Analyse de convergence avec diff√©rentes initializations
4. Liste de findings (P0/P1/P2) avec recommandations
5. Code de validation reproductible
```

**M√©triques de Succ√®s**:
- ‚úÖ K-Means warm start am√©liore la convergence (log-likelihood initiale +20%)
- ‚úÖ Initialization 100% reproductible (m√™me seed ‚Üí m√™me r√©sultat)
- ‚úÖ Noise injection optimal (test grid search)
- ‚úÖ Retry logic justifi√© (am√©liore la qualit√© dans >80% des cas)

---

### √âtape 1.4 : Audit Archetype Alignment

**ID**: `P1.4`  
**D√©pendances**: Aucune  
**Parall√©lisable avec**: P1.1, P1.2, P1.3  
**Score complexit√©**: 7 (optimisation combinatoire + validation m√©tier)

**Prompt Optimis√©**:
```text
## Audit Archetype Alignment (Hungarian Algorithm)

### Persona
Tu es un expert en optimisation combinatoire avec expertise en assignment problems et validation de mod√®les pour la finance.

### Contexte
- M√©thode: Hungarian Algorithm (scipy.optimize.linear_sum_assignment)
- Arch√©types fixes:
  - State 0: Crash (-0.50%/h, 4.0%/h vol)
  - State 1: Downtrend (-0.10%/h, 1.5%/h vol)
  - State 2: Range (0.00%/h, 0.5%/h vol)
  - State 3: Uptrend (+0.15%/h, 2.0%/h vol)
- Distance: Euclidienne pond√©r√©e (w_ret=1.0, w_vol=2.0)
- Fichier: `src/data_engineering/manager.py` (lignes 299-362)

### T√¢che
Auditer l'Archetype Alignment selon les standards SOTA:

1. **Archetype Calibration**
   - Valider que les arch√©types sont r√©alistes pour BTC hourly
   - Comparer avec les r√©gimes observ√©s historiquement (2020-2024)
   - V√©rifier que les arch√©types couvrent bien l'espace des r√©gimes
   - Tester si des arch√©types additionnels sont n√©cessaires

2. **Distance Metric**
   - Valider la distance euclidienne pond√©r√©e
   - V√©rifier que w_vol=2.0 est optimal (vol plus discriminante)
   - Comparer avec d'autres m√©triques (Mahalanobis, cosine)
   - Tester si la normalisation (z-scores) est correcte

3. **Hungarian Algorithm Correctness**
   - V√©rifier que l'algorithme trouve bien l'optimal global
   - Valider que le mapping est unique (pas d'ambigu√Øt√©)
   - Tester les cas edge (√©tats tr√®s proches, arch√©types mal calibr√©s)

4. **Semantic Drift Resolution**
   - Valider que l'alignment r√©sout vraiment le semantic drift
   - Tester la stabilit√© du mapping entre segments WFO
   - V√©rifier que Prob_0 signifie toujours "Crash" entre segments
   - Analyser les cas o√π l'alignment √©choue

5. **Alternative Alignment Methods**
   - Comparer avec d'autres m√©thodes:
     - Maximum likelihood alignment
     - Wasserstein distance
     - Procrustes analysis
   - √âvaluer si une m√©thode alternative est meilleure

6. **Validation M√©tier**
   - V√©rifier que les r√©gimes align√©s ont du sens √©conomiquement
   - Analyser la coh√©rence avec les √©v√©nements de march√© (crashes, bull runs)
   - Valider que les transitions entre r√©gimes sont r√©alistes

### Livrables
1. Rapport d'audit avec validation des arch√©types
2. Tests de stabilit√© du mapping entre segments WFO
3. Comparaison avec m√©thodes alternatives
4. Analyse de coh√©rence m√©tier (√©v√©nements de march√©)
5. Liste de findings (P0/P1/P2) avec recommandations
6. Code de validation reproductible
```

**M√©triques de Succ√®s**:
- ‚úÖ Arch√©types r√©alistes (match avec r√©gimes historiques >80%)
- ‚úÖ Mapping stable entre segments WFO (>95% de coh√©rence)
- ‚úÖ Alignment r√©sout le semantic drift (Prob_0 = Crash toujours)
- ‚úÖ Distance metric optimale (test grid search)

---

### Batch 2 : Audit Validation & Quality

---

### √âtape 2.1 : Audit Quality Metrics

**ID**: `P2.1`  
**D√©pendances**: P1.1, P1.2  
**Parall√©lisable avec**: P2.2, P2.3, P2.4  
**Score complexit√©**: 6 (m√©triques de qualit√©)

**Prompt Optimis√©**:
```text
## Audit HMM Quality Metrics

### Persona
Tu es un expert en validation de mod√®les g√©n√©ratifs avec expertise en m√©triques de qualit√© pour HMM (separation, persistence, interpretability).

### Contexte
- M√©triques actuelles:
  - n_active_states: nombre d'√©tats avec proportion > 5%
  - state_proportions: distribution des √©tats
  - state_mean_returns: mean return par √©tat
  - separation_score: std(mean_returns)
  - is_valid: n_active_states >= 3
- Fichier: `src/data_engineering/manager.py` (lignes 364-419)

### T√¢che
Auditer les m√©triques de qualit√© selon les standards SOTA:

1. **Completeness of Metrics**
   - Identifier les m√©triques manquantes SOTA:
     - Regime persistence (dur√©e moyenne des r√©gimes)
     - Transition matrix entropy
     - State separation (distance entre √©tats)
     - Predictive power (corr√©lation Prob_* avec future returns)
     - Calibration (Prob_* bien calibr√©es?)
   - Proposer une suite compl√®te de m√©triques

2. **Validation Thresholds**
   - Valider min_proportion=0.05 (5% minimum par √©tat)
   - V√©rifier que is_valid (n_active >= 3) est suffisant
   - Tester diff√©rents seuils et leur impact

3. **Separation Score**
   - Valider que std(mean_returns) est une bonne m√©trique
   - Comparer avec d'autres m√©triques de s√©paration:
     - Silhouette score
     - Davies-Bouldin index
     - Distance inter-√©tats (Mahalanobis)
   - Proposer une m√©trique composite

4. **Predictive Power Validation**
   - Tester si Prob_* pr√©dit les future returns (1h, 24h, 168h ahead)
   - Calculer l'information mutuelle entre Prob_* et future returns
   - Valider que le HMM capture bien les r√©gimes pr√©dictifs

5. **Calibration Analysis**
   - V√©rifier que Prob_* sont bien calibr√©es (reliability diagram)
   - Tester si Prob_0 √©lev√© correspond vraiment √† des crashes
   - Analyser les cas de mauvais calibrage

### Livrables
1. Rapport d'audit avec m√©triques SOTA compl√®tes
2. Tests de predictive power (corr√©lation avec future returns)
3. Reliability diagrams (calibration)
4. Liste de findings (P0/P1/P2) avec recommandations
5. Code de validation reproductible
```

**M√©triques de Succ√®s**:
- ‚úÖ Suite compl√®te de m√©triques SOTA impl√©ment√©e
- ‚úÖ Predictive power valid√© (corr√©lation Prob_* vs future returns > 0.1)
- ‚úÖ Calibration valid√©e (reliability diagram proche de la diagonale)
- ‚úÖ Separation score optimal (√©tats bien distincts)

---

### √âtape 2.2 : Audit Model Selection

**ID**: `P2.2`  
**D√©pendances**: P1.2  
**Parall√©lisable avec**: P2.1, P2.3, P2.4  
**Score complexit√©**: 7 (model selection theory)

**Prompt Optimis√©**:
```text
## Audit HMM Model Selection

### Persona
Tu es un statisticien avec expertise en model selection (AIC, BIC, cross-validation) et overfitting detection.

### Contexte
- S√©lection actuelle: n_components=4 fixe (domain knowledge)
- Pas de model selection automatique
- Fichier: `src/data_engineering/manager.py`

### T√¢che
Auditer la strat√©gie de model selection selon les standards SOTA:

1. **Information Criteria**
   - Impl√©menter AIC, BIC, ICL pour HMM
   - Tester n_components ‚àà {2, 3, 4, 5, 6, 7, 8}
   - Valider que n_components=4 est optimal selon les crit√®res
   - Comparer avec la litt√©rature (4 r√©gimes = standard)

2. **Cross-Validation**
   - Impl√©menter time series cross-validation (pas de shuffle)
   - Tester diff√©rents n_components avec CV
   - Valider que n_components=4 minimise l'erreur de pr√©diction

3. **Overfitting Detection**
   - Analyser si n_mix=2 est suffisant (pas d'overfitting)
   - Tester n_mix ‚àà {1, 2, 3, 4}
   - Valider avec train/validation split

4. **Robustness Testing**
   - Tester la stabilit√© du mod√®le sur diff√©rents segments temporels
   - Valider que n_components=4 est robuste (pas de drift)
   - Analyser les cas o√π le mod√®le d√©g√©n√®re

5. **Alternative Model Selection**
   - Comparer avec d'autres m√©thodes:
     - Variational Bayes (automatic model selection)
     - Reversible Jump MCMC
     - Non-parametric HMM
   - √âvaluer si une m√©thode alternative est meilleure

### Livrables
1. Rapport d'audit avec courbes AIC/BIC/ICL
2. R√©sultats de cross-validation
3. Tests d'overfitting (train/validation)
4. Liste de findings (P0/P1/P2) avec recommandations
5. Code de validation reproductible
```

**M√©triques de Succ√®s**:
- ‚úÖ n_components=4 optimal selon AIC/BIC/ICL
- ‚úÖ Cross-validation confirme n_components=4
- ‚úÖ Pas d'overfitting d√©tect√© (n_mix=2 suffisant)
- ‚úÖ Mod√®le robuste sur diff√©rents segments (>90% de stabilit√©)

---

### √âtape 2.3 : Audit Convergence & Stability

**ID**: `P2.3`  
**D√©pendances**: P1.2  
**Parall√©lisable avec**: P2.1, P2.2, P2.4  
**Score complexit√©**: 6 (convergence analysis)

**Prompt Optimis√©**:
```text
## Audit HMM Convergence & Stability

### Persona
Tu es un expert en algorithmes EM avec expertise en analyse de convergence et d√©tection de modes locaux.

### Contexte
- Algorithme: EM (Expectation-Maximization)
- Configuration: n_iter=200, monitor_.tol (default)
- Fichier: `src/data_engineering/manager.py` (lignes 523-526, 561)

### T√¢che
Auditer la convergence et la stabilit√© selon les standards SOTA:

1. **Convergence Analysis**
   - Analyser l'historique de log-likelihood (monitor_.history)
   - Calculer le taux de convergence (monitor_.converged)
   - Identifier les cas de non-convergence
   - Valider que n_iter=200 est suffisant (convergence > 95%)

2. **Local Minima Detection**
   - Tester si l'algorithme EM converge vers des modes locaux
   - Comparer les r√©sultats avec diff√©rentes initializations
   - Analyser la variance des r√©sultats entre runs

3. **Numerical Stability**
   - V√©rifier l'absence de NaN, Inf dans les param√®tres
   - Analyser la condition number de la covariance matrix
   - Tester la stabilit√© avec diff√©rentes √©chelles de donn√©es

4. **Convergence Diagnostics**
   - Impl√©menter des diagnostics SOTA:
     - Geweke diagnostic
     - Raftery-Lewis diagnostic
     - Gelman-Rubin statistic (si multiple chains)
   - Valider que le mod√®le converge bien

5. **Early Stopping**
   - Analyser si early stopping am√©liore la g√©n√©ralisation
   - Tester diff√©rents crit√®res d'arr√™t (tol, patience)
   - Valider que l'early stopping n'emp√™che pas la convergence

### Livrables
1. Rapport d'audit avec analyse de convergence
2. Statistiques de convergence (taux, temps moyen)
3. Tests de stabilit√© num√©rique
4. Diagnostics de convergence SOTA
5. Liste de findings (P0/P1/P2) avec recommandations
6. Code de validation reproductible
```

**M√©triques de Succ√®s**:
- ‚úÖ Taux de convergence > 95% (monitor_.converged)
- ‚úÖ Pas de modes locaux d√©tect√©s (variance entre runs < 5%)
- ‚úÖ Stabilit√© num√©rique valid√©e (pas de NaN/Inf)
- ‚úÖ Diagnostics de convergence SOTA pass√©s

---

### √âtape 2.4 : Audit Retry Logic

**ID**: `P2.4`  
**D√©pendances**: P1.3, P2.1  
**Parall√©lisable avec**: P2.1, P2.2, P2.3  
**Score complexit√©**: 5 (retry heuristics)

**Prompt Optimis√©**:
```text
## Audit HMM Retry Logic

### Persona
Tu es un expert en robustesse algorithmique avec expertise en retry strategies et quality-based selection.

### Contexte
- Retry logic: MAX_RETRIES=3, quality-based selection
- Crit√®re: n_active_states >= 3 (is_valid)
- S√©lection: best = max(n_active_states)
- Fichier: `src/data_engineering/manager.py` (lignes 484-550)

### T√¢che
Auditer la logique de retry selon les standards SOTA:

1. **Retry Strategy**
   - Valider que MAX_RETRIES=3 est optimal
   - Analyser si les retries am√©liorent vraiment la qualit√©
   - Tester diff√©rents nombres de retries (1, 3, 5, 10)

2. **Quality Selection**
   - Valider que max(n_active_states) est le bon crit√®re
   - Comparer avec d'autres crit√®res:
     - Max separation_score
     - Max log-likelihood
     - Composite score (n_active + separation)
   - Proposer un crit√®re optimal

3. **Random State Strategy**
   - Analyser l'impact de random_state + attempt * 17
   - V√©rifier que cette strat√©gie explore bien l'espace
   - Tester si une strat√©gie plus syst√©matique est meilleure

4. **Failure Cases**
   - Analyser les cas o√π tous les retries √©chouent
   - Valider que le fallback (best_hmm) est acceptable
   - Proposer une strat√©gie de fallback am√©lior√©e

5. **Reproducibility Impact**
   - Analyser l'impact des retries sur la reproductibilit√©
   - Valider que le best selection est d√©terministe (m√™me seed ‚Üí m√™me best)
   - Proposer une solution reproductible

### Livrables
1. Rapport d'audit avec analyse des retries
2. Tests de qualit√© avec/sans retries
3. Comparaison de crit√®res de s√©lection
4. Liste de findings (P0/P1/P2) avec recommandations
5. Code de validation reproductible
```

**M√©triques de Succ√®s**:
- ‚úÖ Retries am√©liorent la qualit√© dans >80% des cas
- ‚úÖ Crit√®re de s√©lection optimal (composite score)
- ‚úÖ Retry logic 100% reproductible
- ‚úÖ Fallback strategy robuste

---

### Batch 3 : Audit Data Leakage & WFO

---

### √âtape 3.1 : Audit WFO Integration

**ID**: `P3.1`  
**D√©pendances**: P2.1, P2.2  
**Parall√©lisable avec**: P3.2, P3.3  
**Score complexit√©**: 8 (data leakage critical)

**Prompt Optimis√©**:
```text
## Audit HMM WFO Integration

### Persona
Tu es un expert en data leakage detection avec expertise en walk-forward optimization et temporal validation.

### Contexte
- WFO: fit HMM on train, predict on test
- Fichier: `scripts/run_full_wfo.py` (lignes 389-462)
- Flow: train_hmm() ‚Üí fit_predict(train) ‚Üí predict(eval) ‚Üí predict(test)

### T√¢che
Auditer l'int√©gration WFO selon les standards SOTA:

1. **Data Leakage Detection**
   - V√©rifier que le HMM est bien fit uniquement sur train
   - Valider que predict() utilise le scaler fitt√© sur train
   - Analyser si les features HMM utilisent des donn√©es futures
   - Tester avec un oracle (future data) pour d√©tecter le leakage

2. **Temporal Boundaries**
   - Valider que les segments WFO sont bien s√©par√©s temporellement
   - V√©rifier qu'il n'y a pas de chevauchement entre train/test
   - Analyser l'impact du context buffer sur le leakage

3. **Scaler Consistency**
   - V√©rifier que le scaler est fitt√© uniquement sur train
   - Valider que le scaler est transform√© sur eval/test (pas refit)
   - Tester si le scaler drift entre segments (stationnarit√©)

4. **Model Persistence**
   - Valider que le HMM est bien sauvegard√© et recharg√© correctement
   - V√©rifier que sorted_indices est bien pr√©serv√©
   - Analyser si le mod√®le d√©rive entre segments

5. **Embargo Period**
   - V√©rifier qu'il y a un embargo entre train et test
   - Valider que l'embargo est suffisant (pas de contamination)
   - Analyser l'impact de l'embargo sur la performance

### Livrables
1. Rapport d'audit avec tests de data leakage
2. Tests oracle (future data) pour d√©tecter le leakage
3. Analyse de stationnarit√© du scaler
4. Liste de findings (P0/P1/P2) avec recommandations
5. Code de validation reproductible
```

**M√©triques de Succ√®s**:
- ‚úÖ Aucun data leakage d√©tect√© (tests oracle pass√©s)
- ‚úÖ Scaler fitt√© uniquement sur train (100% des cas)
- ‚úÖ Embargo suffisant (pas de contamination)
- ‚úÖ Mod√®le persiste correctement entre segments

---

### √âtape 3.2 : Audit Context Buffer

**ID**: `P3.2`  
**D√©pendances**: P3.1  
**Parall√©lisable avec**: P3.1, P3.3  
**Score complexit√©**: 6 (context window analysis)

**Prompt Optimis√©**:
```text
## Audit HMM Context Buffer

### Persona
Tu es un expert en time series avec expertise en context windows et lookback requirements.

### Contexte
- Context buffer: 336h (2 semaines) pour eval/test
- HMM window: 168h (1 semaine) pour features
- Fichier: `scripts/run_full_wfo.py` (lignes 416-450)

### T√¢che
Auditer le context buffer selon les standards SOTA:

1. **Context Window Size**
   - Valider que 336h est suffisant pour le HMM (168h window)
   - Analyser si un buffer plus petit suffit (optimisation)
   - Tester diff√©rents tailles de buffer (168h, 336h, 504h)

2. **Lookback Requirements**
   - V√©rifier que toutes les features HMM ont leur lookback satisfait
   - Analyser les features avec le plus long lookback:
     - HMM_Trend: 168h
     - HMM_Vol: 168h
     - HMM_Momentum: 14h (RSI)
     - HMM_RiskOnOff: 168h
     - HMM_VolRatio: 168h (max)
   - Valider que 336h > 168h (safety margin)

3. **Context Buffer Handling**
   - V√©rifier que le context est bien retir√© apr√®s prediction
   - Valider que les indices sont corrects (pas de d√©calage)
   - Analyser si le context contamine les r√©sultats

4. **Edge Cases**
   - Tester le cas o√π le train set est plus court que le buffer
   - Valider que le fallback (min(buffer, len(train))) fonctionne
   - Analyser les cas o√π le buffer est insuffisant

5. **Performance Impact**
   - Analyser l'impact du buffer sur les performances (temps, m√©moire)
   - Optimiser si n√©cessaire (buffer minimal suffisant)

### Livrables
1. Rapport d'audit avec analyse du context buffer
2. Tests de diff√©rentes tailles de buffer
3. Validation des lookback requirements
4. Liste de findings (P0/P1/P2) avec recommandations
5. Code de validation reproductible
```

**M√©triques de Succ√®s**:
- ‚úÖ Context buffer suffisant (336h > 168h max lookback)
- ‚úÖ Pas de contamination du context (indices corrects)
- ‚úÖ Edge cases g√©r√©s (train court, buffer insuffisant)
- ‚úÖ Performance optimale (buffer minimal suffisant)

---

### √âtape 3.3 : Audit Feature Lookback Windows

**ID**: `P3.3`  
**D√©pendances**: P1.1, P3.1  
**Parall√©lisable avec**: P3.1, P3.2  
**Score complexit√©**: 5 (window analysis)

**Prompt Optimis√©**:
```text
## Audit HMM Feature Lookback Windows

### Persona
Tu es un expert en feature engineering avec expertise en rolling windows et temporal dependencies.

### Contexte
- Features HMM avec diff√©rents lookback windows:
  - HMM_Trend: 168h (MA)
  - HMM_Vol: 168h (MA)
  - HMM_Momentum: 14h (RSI)
  - HMM_RiskOnOff: 168h (MA)
  - HMM_VolRatio: 168h (max: vol_long)
- Fichier: `src/data_engineering/manager.py` (lignes 122-194)

### T√¢che
Auditer les lookback windows selon les standards SOTA:

1. **Window Size Justification**
   - Valider que 168h (7 jours) est optimal pour les features de tendance
   - Comparer avec d'autres windows (24h, 72h, 168h, 336h, 720h)
   - Tester si des windows adaptatifs sont meilleurs

2. **Consistency Across Features**
   - Analyser si tous les features devraient avoir le m√™me window
   - Valider que RSI(14h) est coh√©rent avec les autres (168h)
   - Proposer une standardisation si n√©cessaire

3. **Lookback Requirements**
   - V√©rifier que tous les windows sont bien respect√©s (pas de look-ahead)
   - Valider que min_periods est correct (pas de NaN au d√©but)
   - Analyser l'impact des NaN sur le HMM

4. **Multi-Scale Features**
   - Tester si des features multi-scale am√©liorent la d√©tection
   - Comparer avec un HMM multi-timeframe
   - √âvaluer la complexit√© vs b√©n√©fice

5. **Window Optimization**
   - Impl√©menter une recherche optimale des windows
   - Tester avec validation crois√©e temporelle
   - Proposer des windows optimaux

### Livrables
1. Rapport d'audit avec justification des windows
2. Tests de diff√©rents windows (grid search)
3. Analyse de coh√©rence entre features
4. Liste de findings (P0/P1/P2) avec recommandations
5. Code de validation reproductible
```

**M√©triques de Succ√®s**:
- ‚úÖ Windows justifi√©s th√©oriquement (168h = cycle hebdomadaire)
- ‚úÖ Pas de look-ahead bias (windows respect√©s)
- ‚úÖ Windows optimaux (test grid search)
- ‚úÖ Coh√©rence entre features valid√©e

---

### Batch 4 : Audit Robustness & Reproducibility

---

### √âtape 4.1 : Audit Reproducibility

**ID**: `P4.1`  
**D√©pendances**: P1.3, P2.4  
**Parall√©lisable avec**: P4.2  
**Score complexit√©**: 6 (reproducibility engineering)

**Prompt Optimis√©**:
```text
## Audit HMM Reproducibility

### Persona
Tu es un expert en reproductibilit√© scientifique avec expertise en random seeds, determinism, et version control.

### Contexte
- Sources de non-reproductibilit√© identifi√©es:
  - K-Means random_state changeant entre retries
  - HMM random_state changeant entre retries
  - Noise injection avec np.random.seed()
- Fichier: `src/data_engineering/manager.py` (lignes 489-521)

### T√¢che
Auditer la reproductibilit√© selon les standards SOTA:

1. **Random Seed Management**
   - Identifier toutes les sources d'al√©a:
     - K-Means random_state
     - HMM random_state
     - Noise injection (np.random)
     - Scikit-learn internals
   - Impl√©menter un seed manager centralis√©
   - Valider que le m√™me seed produit le m√™me r√©sultat

2. **Determinism Testing**
   - Impl√©menter des tests de d√©terminisme:
     - M√™me seed ‚Üí m√™me r√©sultat (100%)
     - Diff√©rents seeds ‚Üí r√©sultats diff√©rents mais coh√©rents
   - Valider que tous les chemins de code sont d√©terministes

3. **Version Control**
   - Documenter les versions de toutes les d√©pendances
   - Valider que les r√©sultats sont reproductibles entre versions
   - Impl√©menter des tests de r√©gression

4. **Numerical Precision**
   - Analyser l'impact de la pr√©cision num√©rique (float32 vs float64)
   - Valider que les r√©sultats sont stables (pas de drift)
   - Tester sur diff√©rentes plateformes (CPU, GPU)

5. **Reproducibility Report**
   - G√©n√©rer un rapport de reproductibilit√© automatique
   - Inclure: seed, versions, hardware, r√©sultats
   - Valider que le rapport est complet

### Livrables
1. Rapport d'audit avec tests de reproductibilit√©
2. Seed manager centralis√©
3. Tests de d√©terminisme (100% pass rate)
4. Rapport de reproductibilit√© automatique
5. Liste de findings (P0/P1/P2) avec recommandations
6. Code de validation reproductible
```

**M√©triques de Succ√®s**:
- ‚úÖ 100% de reproductibilit√© (m√™me seed ‚Üí m√™me r√©sultat)
- ‚úÖ Seed manager centralis√© impl√©ment√©
- ‚úÖ Tests de d√©terminisme pass√©s (100% pass rate)
- ‚úÖ Rapport de reproductibilit√© automatique g√©n√©r√©

---

### √âtape 4.2 : Audit Edge Cases & Numerical Stability

**ID**: `P4.2`  
**D√©pendances**: P1.2, P2.3  
**Parall√©lisable avec**: P4.1  
**Score complexit√©**: 7 (edge cases + numerical analysis)

**Prompt Optimis√©**:
```text
## Audit HMM Edge Cases & Numerical Stability

### Persona
Tu es un expert en robustesse algorithmique avec expertise en edge cases, numerical stability, et error handling.

### Contexte
- Cas edge identifi√©s:
  - Donn√©es insuffisantes (< 100 samples)
  - Features avec NaN/Inf
  - Covariance matrix singuli√®re
  - Non-convergence EM
- Fichier: `src/data_engineering/manager.py`

### T√¢che
Auditer les edge cases et la stabilit√© num√©rique selon les standards SOTA:

1. **Data Quality Edge Cases**
   - Tester avec donn√©es insuffisantes (< 100 samples)
   - Valider que l'erreur est bien lev√©e (ValueError)
   - Tester avec donn√©es tr√®s courtes (juste le minimum)
   - Analyser le comportement avec donn√©es corrompues

2. **Numerical Stability**
   - Tester avec features extr√™mes (tr√®s grandes, tr√®s petites)
   - Valider que le clipping pr√©vient les probl√®mes
   - Analyser la condition number de la covariance matrix
   - Tester avec donn√©es d√©g√©n√©r√©es (variance = 0)

3. **NaN/Inf Handling**
   - Tester avec NaN dans les features
   - Valider que valid_mask filtre correctement
   - Analyser le comportement avec Inf
   - V√©rifier que le HMM ne produit pas de NaN/Inf

4. **Convergence Edge Cases**
   - Tester avec donn√©es qui ne convergent pas
   - Valider que le retry logic g√®re bien ces cas
   - Analyser le fallback (best_hmm)
   - Proposer une strat√©gie am√©lior√©e

5. **Covariance Matrix Issues**
   - Tester avec covariance matrix singuli√®re
   - Valider que min_covar pr√©vient les probl√®mes
   - Analyser les cas o√π la r√©gularisation √©choue
   - Proposer une solution robuste

6. **Memory & Performance**
   - Tester avec tr√®s grandes datasets (OOM?)
   - Analyser la complexit√© algorithmique
   - Optimiser si n√©cessaire

### Livrables
1. Rapport d'audit avec tests d'edge cases
2. Tests de stabilit√© num√©rique (extreme values)
3. Tests de NaN/Inf handling
4. Tests de convergence edge cases
5. Liste de findings (P0/P1/P2) avec recommandations
6. Code de validation reproductible
```

**M√©triques de Succ√®s**:
- ‚úÖ Tous les edge cases g√©r√©s (pas de crash)
- ‚úÖ Stabilit√© num√©rique valid√©e (pas de NaN/Inf)
- ‚úÖ Error handling robuste (messages clairs)
- ‚úÖ Performance acceptable (pas d'OOM)

---

### Batch 5 : Synth√®se & Recommandations

---

### √âtape 5 : Synth√®se & Recommandations

**ID**: `P5`  
**D√©pendances**: P1, P2, P3, P4  
**Score complexit√©**: 8 (synthesis + prioritization)

**Prompt Optimis√©**:
```text
## Synth√®se & Recommandations - Audit HMM SOTA

### Persona
Tu es un architecte technique senior avec expertise en synth√®se d'audits, priorisation, et roadmap planning.

### Contexte
- Audits compl√©t√©s: P1.1-P1.4, P2.1-P2.4, P3.1-P3.3, P4.1-P4.2
- Findings collect√©s: P0 (critiques), P1 (importants), P2 (mineurs)
- Objectif: Synth√®se, priorisation, roadmap

### T√¢che
Synth√©tiser tous les audits et produire un rapport final:

1. **Executive Summary**
   - Score global HMM (0-10)
   - Top 5 findings critiques
   - Recommandations prioritaires
   - Impact estim√© des corrections

2. **Findings Aggregation**
   - Regrouper les findings par cat√©gorie:
     - Architecture & Design
     - Validation & Quality
     - Data Leakage & WFO
     - Robustness & Reproducibility
   - Prioriser (P0 > P1 > P2)
   - Estimer l'effort de correction

3. **Risk Matrix**
   - Probabilit√© vs Impact pour chaque finding
   - Identifier les risques critiques
   - Proposer un plan de mitigation

4. **Roadmap de Correction**
   - Phase 1: P0 (bloquants) - 1-2 semaines
   - Phase 2: P1 (importants) - 2-4 semaines
   - Phase 3: P2 (am√©liorations) - 4-8 semaines
   - D√©pendances entre corrections

5. **M√©triques de Succ√®s**
   - D√©finir des KPIs pour valider les corrections
   - Impl√©menter des tests de r√©gression
   - Valider que les corrections am√©liorent la qualit√©

6. **Comparaison SOTA**
   - Comparer avec les impl√©mentations SOTA (fHMM, etc.)
   - Identifier les gaps
   - Proposer des am√©liorations futures

### Livrables
1. Rapport de synth√®se complet (Executive Summary)
2. Risk matrix avec priorisation
3. Roadmap de correction (phases, d√©pendances)
4. M√©triques de succ√®s (KPIs)
5. Comparaison SOTA (gaps, am√©liorations)
6. Code de validation (tests de r√©gression)
```

**M√©triques de Succ√®s**:
- ‚úÖ Score global HMM > 8/10
- ‚úÖ Tous les P0 corrig√©s (0 findings critiques)
- ‚úÖ Roadmap claire avec d√©pendances
- ‚úÖ M√©triques de succ√®s d√©finies et mesurables

---

## üìä Matrice de Risque (Template)

| ID | Finding | Prob | Impact | Priority | Effort | Status |
|----|---------|------|--------|----------|--------|--------|
| P1.1-X | Feature X has look-ahead bias | H/M/L | H/M/L | P0/P1/P2 | S/M/L | ‚è≥/‚úÖ/‚ùå |
| ... | ... | ... | ... | ... | ... | ... |

**L√©gende**:
- **Prob**: Probabilit√© (H=High, M=Medium, L=Low)
- **Impact**: Impact sur la qualit√© (H=High, M=Medium, L=Low)
- **Priority**: P0 (Bloquant), P1 (Important), P2 (Am√©lioration)
- **Effort**: S (Small <1j), M (Medium 1-3j), L (Large >3j)
- **Status**: ‚è≥ (√Ä faire), ‚úÖ (Fait), ‚ùå (Rejet√©)

---

## üéØ M√©triques de Succ√®s Globales

1. **Score Global HMM**: > 8/10
2. **Findings P0**: 0 (tous corrig√©s)
3. **Reproducibilit√©**: 100% (m√™me seed ‚Üí m√™me r√©sultat)
4. **Data Leakage**: 0 d√©tect√© (tests oracle pass√©s)
5. **Convergence Rate**: > 95% (monitor_.converged)
6. **Predictive Power**: Corr√©lation Prob_* vs future returns > 0.1
7. **Calibration**: Reliability diagram proche de la diagonale
8. **Stabilit√© WFO**: Mapping coh√©rent entre segments > 95%

---

## üìö R√©f√©rences SOTA

1. **Hamilton (1989)**: "A New Approach to the Economic Analysis of Nonstationary Time Series"
2. **Rabiner (1989)**: "A Tutorial on Hidden Markov Models"
3. **Ang & Timmermann (2002)**: "Regime Changes and Financial Markets"
4. **Shu et al. (2024)**: "Statistical Jump Models for Regime Detection"
5. **fHMM (R)**: "Hidden Markov Models for Financial Time Series"
6. **Lopez de Prado (2018)**: "Advances in Financial Machine Learning"

---

## ‚úÖ Checklist d'Ex√©cution

### Phase 1: Architecture & Design
- [ ] P1.1: Audit Features Engineering
- [ ] P1.2: Audit GMM-HMM Configuration
- [ ] P1.3: Audit Initialization Strategy
- [ ] P1.4: Audit Archetype Alignment

### Phase 2: Validation & Quality
- [ ] P2.1: Audit Quality Metrics
- [ ] P2.2: Audit Model Selection
- [ ] P2.3: Audit Convergence & Stability
- [ ] P2.4: Audit Retry Logic

### Phase 3: Data Leakage & WFO
- [ ] P3.1: Audit WFO Integration
- [ ] P3.2: Audit Context Buffer
- [ ] P3.3: Audit Feature Lookback Windows

### Phase 4: Robustness & Reproducibility
- [ ] P4.1: Audit Reproducibility
- [ ] P4.2: Audit Edge Cases & Numerical Stability

### Phase 5: Synth√®se
- [ ] P5: Synth√®se & Recommandations

---

**Date de cr√©ation**: 2026-01-22  
**Derni√®re mise √† jour**: 2026-01-22  
**Version**: 1.0
