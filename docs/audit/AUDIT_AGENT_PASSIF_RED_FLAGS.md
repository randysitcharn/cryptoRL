# Audit "Deep Dive" - Agent Passif (Attribution 0.0, Action Mean 0.0)

**Date:** 2026-01-24  
**Mission:** Identifier les causes de la passivit√© de l'agent (Attribution 0.0, Action Mean 0.0)

---

## üî¥ RED FLAG #1 : D√©s√©quilibre Massif Reward/Cost (CRITIQUE)

### Localisation
`src/training/batch_env.py`, ligne 424-507 (`_calculate_rewards`)

### Probl√®me Identifi√©

**Calcul du co√ªt d'un changement de position de 0 ‚Üí 1 :**

```python
# Ligne 477
r_cost = -position_deltas * SCALE  # SCALE = 10.0
# Pour delta = 1.0 : r_cost = -1.0 * 10 = -10.0

# Ligne 481 : Clamp√© √† COST_PENALTY_CAP = 2.0
r_cost = torch.clamp(r_cost, min=-COST_PENALTY_CAP, max=0.0)  # ‚Üí -2.0

# Ligne 493 : Application MORL avec w_cost=1 et MAX_PENALTY_SCALE=0.4
reward = r_perf + (w_cost_squeezed * r_cost * MAX_PENALTY_SCALE)
# ‚Üí reward = r_perf + (1.0 * -2.0 * 0.4) = r_perf - 0.8
```

**Comparaison avec un return moyen de 0.1% (0.001) :**

```python
# Ligne 469
r_perf = torch.log1p(safe_returns) * SCALE
# Pour return = 0.001 : r_perf = log1p(0.001) * 10 ‚âà 0.00998 ‚âà 0.01
```

### Impact

**Ratio P√©nalit√©/Gain = -0.8 / 0.01 = -80x**

L'agent est **instantan√©ment puni 80 fois plus fort** qu'il ne peut gagner en moyenne. M√™me avec `w_cost=0` (scalping mode), si l'agent explore et que certains envs ont `w_cost>0`, il subit des p√©nalit√©s massives.

### Preuve dans le Code

```python:424:507:src/training/batch_env.py
SCALE = 10.0
MAX_PENALTY_SCALE = 0.4
COST_PENALTY_CAP = 2.0

# Pour un changement de position de 0 √† 1 :
r_cost = -1.0 * 10.0 = -10.0  # Clamp√© √† -2.0
penalty = -2.0 * 0.4 = -0.8   # Avec w_cost=1

# Pour un return de 0.1% :
r_perf = log1p(0.001) * 10 ‚âà 0.01

# Ratio = -0.8 / 0.01 = -80x (P√âNALIT√â 80x PLUS FORTE)
```

### Recommandation

**Option A (Recommand√©e) :** R√©duire `COST_PENALTY_CAP` de 2.0 √† 0.1
```python
COST_PENALTY_CAP = 0.1  # Au lieu de 2.0
# Nouvelle p√©nalit√© max : -0.1 * 0.4 = -0.04
# Ratio = -0.04 / 0.01 = -4x (acceptable pour exploration)
```

**Option B :** R√©duire `MAX_PENALTY_SCALE` de 0.4 √† 0.05
```python
MAX_PENALTY_SCALE = 0.05  # Au lieu de 0.4
# P√©nalit√© max : -2.0 * 0.05 = -0.1
# Ratio = -0.1 / 0.01 = -10x (encore √©lev√© mais mieux)
```

**Option C (Hybride) :** Combiner les deux
```python
COST_PENALTY_CAP = 0.2
MAX_PENALTY_SCALE = 0.2
# P√©nalit√© max : -0.2 * 0.2 = -0.04
# Ratio = -0.04 / 0.01 = -4x
```

---

## üü° RED FLAG #2 : curriculum_lambda Non Utilis√© (Incoh√©rence)

### Localisation
`src/training/batch_env.py`, ligne 1129-1152 (`set_progress`)

### Probl√®me Identifi√©

`curriculum_lambda` est calcul√© et mis √† jour mais **jamais utilis√© dans `_calculate_rewards`**.

```python
# Ligne 1143-1152 : curriculum_lambda est mis √† jour
if self.progress <= 0.15:
    self.curriculum_lambda = 0.0
elif self.progress <= 0.75:
    self.curriculum_lambda = 0.4 * phase_progress
else:
    self.curriculum_lambda = 0.4

# MAIS ligne 493 : curriculum_lambda n'est PAS utilis√©
reward = r_perf + (w_cost_squeezed * r_cost * MAX_PENALTY_SCALE)
# Devrait √™tre :
# reward = r_perf + (w_cost_squeezed * r_cost * MAX_PENALTY_SCALE * self.curriculum_lambda)
```

### Impact

- Les co√ªts sont appliqu√©s **imm√©diatement** d√®s le d√©but de l'entra√Ænement (m√™me si `w_cost=0` dans certains envs)
- Le curriculum learning ne fonctionne pas comme pr√©vu
- L'agent subit des p√©nalit√©s maximales d√®s le d√©but, m√™me en phase d'exploration

### Recommandation

**Int√©grer `curriculum_lambda` dans le calcul de reward :**

```python
# Ligne 493, modifier en :
effective_penalty_scale = MAX_PENALTY_SCALE * self.curriculum_lambda
reward = r_perf + (w_cost_squeezed * r_cost * effective_penalty_scale)
```

Cela permettrait :
- Phase 1 (0-15%) : `curriculum_lambda=0.0` ‚Üí Pas de p√©nalit√© (exploration pure)
- Phase 2 (15-75%) : `curriculum_lambda` monte progressivement ‚Üí Introduction graduelle des co√ªts
- Phase 3 (75-100%) : `curriculum_lambda=0.4` ‚Üí P√©nalit√©s compl√®tes

---

## üü¢ V√âRIFICATION #1 : INPUT SPLIT (OK)

### Localisation
`src/models/rl_adapter.py`, ligne 100-107 et 320-328

### Analyse

**Calcul de `mae_input_dim` :**
```python
# Ligne 103
if self.use_film:
    self.mae_input_dim = self.n_features - HMM_CONTEXT_SIZE  # 43 - 5 = 38
```

**Slicing dans `forward` :**
```python
# Ligne 323
mae_obs = market_obs[:, :, :-HMM_CONTEXT_SIZE]  # (B, 64, 38) ‚úì
hmm_context = market_obs[:, -1, -HMM_CONTEXT_SIZE:].float()  # (B, 5) ‚úì
```

**Verdict :** ‚úÖ Le slicing est **robuste** et coh√©rent. `mae_input_dim` est calcul√© correctement pour matcher les poids pr√©-entra√Æn√©s.

---

## üü¢ V√âRIFICATION #2 : CONTR√îLE POLICY (OK)

### Localisation
`src/models/tqc_dropout_policy.py`

### Analyse

**1. Spectral Normalization sur Actor :**
```python
# Ligne 340
use_spectral_norm_actor: bool = False,  # Default False (conservative)
```
‚úÖ **OK** : `spectral_norm` n'est **pas** appliqu√© √† l'Actor par d√©faut (configurable via `use_spectral_norm_actor`).

**2. Valeur par d√©faut de `log_std_init` :**
```python
# Ligne 131 (DropoutActor)
log_std_init: float = -1.0,  # FIX: Hardcoded -1 for larger positions (was -3)

# Ligne 342 (TQCDropoutPolicy)
log_std_init: float = -1.0,  # FIX: -1 gives std‚âà0.37 (vs SB3 default -3 giving std‚âà0.05)

# Ligne 91 (training.py)
log_std_init: float = -1.0  # FIX: Increased init exploration
```
‚úÖ **OK** : `log_std_init = -1.0` donne `std ‚âà 0.37` (vs `-3.0` ‚Üí `std ‚âà 0.05`), ce qui est **correct** pour l'exploration.

**Verdict :** ‚úÖ Aucun probl√®me d√©tect√©. La policy est correctement configur√©e pour l'exploration.

---

## üü¢ V√âRIFICATION #3 : FREEZE STATUS (OK)

### Localisation
`src/config/training.py`, ligne 68  
`src/models/rl_adapter.py`, ligne 60

### Analyse

```python
# training.py ligne 68
freeze_encoder: bool = True  # Par d√©faut

# rl_adapter.py ligne 60
freeze_encoder: bool = True,  # Param√®tre par d√©faut
```

**Verdict :** ‚úÖ `freeze_encoder = True` par d√©faut, ce qui est **attendu** pour pr√©server les repr√©sentations pr√©-entra√Æn√©es. Ce n'est **pas** la cause de la passivit√©.

---

## üìä R√âSUM√â DES RED FLAGS

| # | S√©v√©rit√© | Localisation | Probl√®me | Impact |
|---|-----------|--------------|----------|--------|
| **1** | üî¥ **CRITIQUE** | `batch_env.py:477-493` | D√©s√©quilibre Reward/Cost (-80x) | Agent puni 80x plus fort qu'il ne peut gagner ‚Üí **Passivit√© totale** |
| **2** | üü° **MOYEN** | `batch_env.py:493` | `curriculum_lambda` non utilis√© | P√©nalit√©s appliqu√©es d√®s le d√©but, pas de phase d'exploration |

### Actions Recommand√©es (Priorit√©)

1. **URGENT** : R√©duire `COST_PENALTY_CAP` de 2.0 √† 0.1 dans `batch_env.py:462`
2. **URGENT** : Int√©grer `curriculum_lambda` dans le calcul de reward (ligne 493)
3. **Optionnel** : Ajuster `MAX_PENALTY_SCALE` si n√©cessaire apr√®s test

### Test de Validation

Apr√®s corrections, v√©rifier dans TensorBoard :
- `reward/pnl_component` : Devrait √™tre positif en moyenne
- `reward/churn_cost` : Devrait √™tre n√©gatif mais **proportionnel** √† `r_perf`
- `action_mean` : Devrait sortir de 0.0 apr√®s quelques milliers de steps
- `curriculum/lambda` : Devrait √™tre 0.0 en d√©but d'entra√Ænement

---

## üîç CALCULS D√âTAILL√âS (RED FLAG #1)

### Sc√©nario : Agent change de position de 0 √† 1

**Inputs :**
- `position_deltas = 1.0`
- `step_returns = 0.001` (0.1% return moyen)
- `w_cost = 1.0` (worst case, B&H mode)
- `SCALE = 10.0`
- `MAX_PENALTY_SCALE = 0.4`
- `COST_PENALTY_CAP = 2.0`

**Calculs :**

```python
# 1. Performance reward
r_perf = log1p(0.001) * 10.0
r_perf = 0.000998 * 10.0 ‚âà 0.01

# 2. Cost penalty
r_cost = -1.0 * 10.0 = -10.0
r_cost = clamp(-10.0, min=-2.0, max=0.0) = -2.0

# 3. MORL scalarization
penalty = 1.0 * (-2.0) * 0.4 = -0.8

# 4. Total reward
reward = 0.01 + (-0.8) = -0.79
```

**R√©sultat :** L'agent re√ßoit une r√©compense de **-0.79** pour un changement de position, m√™me avec un return positif de 0.1%.

**Ratio P√©nalit√©/Gain :** `-0.8 / 0.01 = -80x`

### Sc√©nario : Agent reste en cash (position = 0)

**Inputs :**
- `position_deltas = 0.0`
- `step_returns = 0.001`
- `w_cost = 1.0`

**Calculs :**

```python
r_perf = 0.01
r_cost = 0.0
penalty = 0.0
reward = 0.01 + 0.0 = 0.01  # POSITIF !
```

**R√©sultat :** L'agent re√ßoit une r√©compense de **+0.01** en restant en cash.

**Conclusion :** L'agent apprend que **rester en cash (reward = +0.01) est meilleur que trader (reward = -0.79)**. C'est la cause directe de la passivit√©.

---

## üéØ CORRECTIONS PROPOS√âES

### Correction #1 : R√©duire COST_PENALTY_CAP

```python
# batch_env.py ligne 462
COST_PENALTY_CAP = 0.1  # Au lieu de 2.0
```

**Nouveau calcul :**
```python
r_cost = clamp(-10.0, min=-0.1, max=0.0) = -0.1
penalty = 1.0 * (-0.1) * 0.4 = -0.04
reward = 0.01 + (-0.04) = -0.03
```

**Ratio :** `-0.04 / 0.01 = -4x` (acceptable pour exploration)

### Correction #2 : Int√©grer curriculum_lambda

```python
# batch_env.py ligne 493
effective_penalty_scale = MAX_PENALTY_SCALE * self.curriculum_lambda
reward = r_perf + (w_cost_squeezed * r_cost * effective_penalty_scale)
```

**Phase 1 (0-15%) :** `curriculum_lambda=0.0` ‚Üí Pas de p√©nalit√©
**Phase 2 (15-75%) :** `curriculum_lambda` monte progressivement
**Phase 3 (75-100%) :** `curriculum_lambda=0.4` ‚Üí P√©nalit√©s compl√®tes

---

**Fin du rapport d'audit**
