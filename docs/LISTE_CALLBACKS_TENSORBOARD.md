# Liste des Callbacks TensorBoard

Ce document liste tous les callbacks et m√©canismes de logging TensorBoard dans le projet cryptoRL.

## üìã Vue d'ensemble

Le projet utilise une architecture unifi√©e pour le logging TensorBoard :
1. **UnifiedMetricsCallback** : Callback principal utilisant uniquement `logger.record()` / `logger.record_mean()` (automatiquement logg√©s par SB3)
2. **SummaryWriter direct** : Utilis√© uniquement pour HMM/MAE training (hors scope RL)

**Architecture "Single Source of Truth"** : Toutes les m√©triques RL passent par le logger SB3 pour garantir la synchronisation des timesteps et √©viter les doublons.

---

## üîÑ Callbacks SB3 (via logger.record)

Ces callbacks utilisent le syst√®me de logging de Stable-Baselines3, qui √©crit automatiquement dans TensorBoard.

### 1. **UnifiedMetricsCallback** ‚≠ê NOUVEAU
**Fichier** : `src/training/callbacks.py` (lignes 134-465)

**Description** : Callback unifi√© rempla√ßant `TensorBoardStepCallback`, `StepLoggingCallback`, et `DetailTensorboardCallback`. Centralise tout le logging via `logger.record()` avec des namespaces standardis√©s.

**Architecture optimis√©e** :
- **M√©triques l√©g√®res** (chaque step) : Buffer pour lissage (NAV, position, drawdown)
- **M√©triques lourdes** (uniquement √† `log_freq`) : Gradients, Q-values TQC, `get_global_metrics()`

**M√©triques logg√©es (√âpur√©es - Signal uniquement)** :

**Portfolio** :
- `portfolio/nav` - Portfolio NAV
- `portfolio/position_pct` - Position en pourcentage

**Risk** :
- `risk/max_drawdown` - Max drawdown en pourcentage

**Rewards (Agr√©g√©es)** :
- `rewards/pnl_component` - Composante PnL du reward (signal principal)
- `rewards/total_penalties` - Somme agr√©g√©e de toutes les p√©nalit√©s (churn + smoothness + downside_risk)
- ‚ùå Composantes individuelles supprim√©es (trop granulaires)

**Strategy** :
- `strategy/churn_ratio` - Ratio churn/PnL par √©pisode

**Debug TQC (Essentielles pour diagnostic Gamma)** :
- `debug/q_values_mean` - Moyenne des Q-values TQC
- `debug/q_values_std` - √âcart-type des Q-values TQC
- `debug/grad_actor_norm` - Norme L2 des gradients de l'actor
- `debug/grad_critic_norm` - Norme L2 des gradients du critic

**Fr√©quence** :
- M√©triques l√©g√®res : Buffer √† chaque step, logg√©es √† `log_freq`
- M√©triques lourdes : Calcul√©es et logg√©es uniquement √† `log_freq` (performance)

**Optimisations** :
- Utilise `logger.record_mean()` pour les m√©triques bufferis√©es (courbes lisses)
- Calcul des gradients uniquement √† `log_freq` (√©vite 10-20% de ralentissement)
- Polling direct GPU via `get_global_metrics()` (pas via infos)
- Monitoring TQC int√©gr√© pour diagnostic Gamma (Q-values)

**Console logging** : Optionnel (flag `verbose`), format identique √† l'ancien `StepLoggingCallback`

**M√©triques supprim√©es (bruit)** :
- ‚ùå `portfolio/nav_std` (redondant avec max_drawdown)
- ‚ùå `strategy/price` (bruit visuel, prix normalis√©s)
- ‚ùå `rewards/churn_cost`, `rewards/smoothness`, `rewards/downside_risk` (agr√©g√©es en `total_penalties`)
- ‚ùå `debug/q_values_min`, `debug/q_values_max` (outliers, std suffit)
- ‚ùå `time/fps_live` (si doublon avec SB3)

---

### 2. **ThreePhaseCurriculumCallback**
**Fichier** : `src/training/callbacks.py` (lignes 618-684)

**M√©triques logg√©es** :
- `curriculum/phase` - Phase actuelle (1, 2, ou 3)
- `curriculum/progress` - Progression totale (0.0 √† 1.0)
- `curriculum/lambda` - Valeur de `curriculum_lambda` de l'environnement
- `observation_noise/effective_scale` - √âchelle effective du bruit d'observation

**Fr√©quence** : √Ä chaque step

**Phases** :
- Phase 1 : 0-15% (Discovery)
- Phase 2 : 15-75% (Discipline)
- Phase 3 : 75-100% (Refinement)

---

### 3. **OverfittingGuardCallbackV2**
**Fichier** : `src/training/callbacks.py` (lignes 868-1318)

**M√©triques logg√©es** :
- `overfit/max_nav_ratio` - Ratio NAV max / NAV initial
- `overfit/weight_delta` - Delta moyen des poids
- `overfit/weight_cv` - Coefficient de variation des poids
- `overfit/train_eval_divergence` - Divergence train/eval
- `overfit/action_saturation` - Ratio d'actions satur√©es
- `overfit/reward_variance` - Variance des rewards
- `overfit/reward_cv` - Coefficient de variation des rewards
- `overfit/violations_{name}` - Compteurs de violations par signal
- `overfit/active_signals` - Nombre de signaux actifs

**Fr√©quence** : Tous les `check_freq` steps (d√©faut: 10,000)

**Signaux de d√©tection** :
1. NAV threshold (retours irr√©alistes)
2. Weight stagnation (convergence/collapse)
3. Train/Eval divergence (overfitting classique)
4. Action saturation (collapse de la politique)
5. Reward variance (m√©morisation)

---

### 4. **ModelEMACallback**
**Fichier** : `src/training/callbacks.py` (lignes 1324-1500)

**M√©triques logg√©es** :
- `ema/weight_diff_l2` - Diff√©rence L2 entre poids actuels et EMA

**Fr√©quence** : Tous les 10,000 steps

**Fonctionnalit√©** : Maintient une copie EMA (Exponential Moving Average) des poids du mod√®le pour √©viter l'overfitting.

---

## üìä SummaryWriter Direct (Hors Scope RL)

Ces m√©canismes utilisent `SummaryWriter` directement pour des cas sp√©cifiques (HMM, MAE) qui ne sont pas dans le scope du training RL.

### 5. **train_foundation.py (MAE Training)**
**Fichier** : `src/training/train_foundation.py` (lignes 466-551)

**M√©triques logg√©es** :
- `loss/train_total` - Loss totale d'entra√Ænement
- `loss/train_recon` - Loss de reconstruction
- `loss/train_aux` - Loss auxiliaire (si supervised)
- `loss/val_total` - Loss totale de validation
- `loss/val_recon` - Loss de reconstruction (validation)
- `loss/val_aux` - Loss auxiliaire (validation)
- `loss/best_val` - Meilleure loss de validation
- `time/epoch_seconds` - Temps par epoch
- `accuracy/val_direction` - Pr√©cision de direction (si supervised)
- `hparam/*` - Hyperparam√®tres (via `add_hparams`)

**Fr√©quence** : √Ä chaque epoch

**Contexte** : Entra√Ænement du mod√®le MAE (Masked Autoencoder).

---

### 6. **DataManager.fit_predict (HMM Training)**
**Fichier** : `src/data_engineering/manager.py` (lignes 460-689)

**M√©triques logg√©es** :
- `hmm/log_likelihood` - Log-likelihood par it√©ration EM
- `hmm/log_likelihood_delta` - Delta de log-likelihood
- `hmm/final/converged` - Statut de convergence (0/1)
- `hmm/final/n_iterations` - Nombre d'it√©rations EM
- `hmm/final/kmeans_inertia` - Inertie K-Means
- `hmm/final/log_likelihood` - Log-likelihood final
- `hmm/final/transmat_entropy` - Entropie de la matrice de transition
- `hmm/final/transmat_diag_avg` - Moyenne de la diagonale (persistance)
- `hmm/final/transition_penalty` - P√©nalit√© de transition appliqu√©e
- `hmm/state_{i}/annual_return_pct` - Return annuel par √©tat
- `hmm/state_{i}/distribution_pct` - Distribution en pourcentage par √©tat

**Fr√©quence** : 
- Par it√©ration EM pour `log_likelihood`
- Par segment WFO pour les m√©triques finales

**Contexte** : Entra√Ænement du HMM (Hidden Markov Model) pour la d√©tection de r√©gimes.

---

### 7. **run_full_wfo.py (Evaluation)**
**Fichier** : `scripts/run_full_wfo.py` (lignes 997-1293)

**M√©triques logg√©es** :

**√âvaluation Ensemble** :
- `eval_ensemble/sharpe` - Ratio de Sharpe
- `eval_ensemble/pnl_pct` - PnL en pourcentage
- `eval_ensemble/max_drawdown` - Max drawdown
- `eval_ensemble/avg_agreement` - Accord moyen entre mod√®les
- `eval_ensemble/avg_std` - √âcart-type moyen des pr√©dictions
- `eval_ensemble/alpha` - Alpha (retour ajust√© au risque)

**√âvaluation Standard** :
- `eval/sharpe` - Ratio de Sharpe
- `eval/pnl_pct` - PnL en pourcentage
- `eval/max_drawdown` - Max drawdown
- `eval/total_trades` - Nombre total de trades
- `eval/circuit_breakers` - Nombre de circuit breakers d√©clench√©s
- `eval/final_nav` - NAV final

**Fr√©quence** : Par segment WFO (segment_id comme step)

**Contexte** : √âvaluation des mod√®les pendant le Walk-Forward Optimization.

---

## üìù M√©triques SB3 Standard

Stable-Baselines3 log automatiquement ces m√©triques (sans callback personnalis√©) :

- `rollout/ep_rew_mean` - Reward moyen par √©pisode
- `rollout/ep_len_mean` - Longueur moyenne d'√©pisode
- `train/actor_loss` - Loss de l'actor
- `train/critic_loss` - Loss du critic
- `train/ent_coef` - Coefficient d'entropie
- `train/ent_coef_loss` - Loss du coefficient d'entropie
- `train/learning_rate` - Taux d'apprentissage
- `train/n_updates` - Nombre de mises √† jour
- `train/policy_gradient_loss` - Loss du gradient de politique
- `train/value_loss` - Loss de la valeur
- `time/fps` - FPS (peut √™tre 0 avec BatchCryptoEnv, d'o√π `time/fps_live`)

---

## üéØ Utilisation dans le Code

### Cr√©ation des Callbacks

Les callbacks sont cr√©√©s dans `src/training/train_agent.py` via `create_callbacks()` :

```python
callbacks = [
    UnifiedMetricsCallback(log_freq=config.log_freq, verbose=config.verbose),  # ‚≠ê NOUVEAU
    ThreePhaseCurriculumCallback(total_timesteps=config.total_timesteps),
    EvalCallbackWithNoiseControl(...),  # Pas de logging TensorBoard direct
    RotatingCheckpointCallback(...),    # Pas de logging TensorBoard
    OverfittingGuardCallbackV2(...),      # Si activ√©
    ModelEMACallback(...),               # Si activ√©
]
```

**Migration** : `UnifiedMetricsCallback` remplace `StepLoggingCallback` et `DetailTensorboardCallback`.

### Configuration TensorBoard

Les chemins de logs sont configur√©s dans :
- `src/config/training.py` : `tensorboard_log` (d√©faut: `"logs/tensorboard_tqc/"`)
- `src/config/base.py` : `tensorboard_log` (d√©faut: `"logs/tensorboard/"`)

### Visualisation

Pour visualiser les logs TensorBoard :

```bash
tensorboard --logdir logs/wfo --port 8081
```

---

## üìä R√©sum√© par Cat√©gorie

| Cat√©gorie | Callback | M√©triques Principales |
|-----------|----------|----------------------|
| **Unifi√©** | UnifiedMetricsCallback ‚≠ê | Portfolio, Risk, Rewards, Strategy, Debug TQC |
| **Curriculum** | ThreePhaseCurriculumCallback | Phase, lambda, noise |
| **Overfitting** | OverfittingGuardCallbackV2 | 5 signaux de d√©tection |
| **EMA** | ModelEMACallback | Diff√©rence poids |
| **HMM** | DataManager.fit_predict | Convergence, √©tats, transitions |
| **MAE** | train_foundation.py | Loss, accuracy |
| **Evaluation** | run_full_wfo.py | Sharpe, PnL, drawdown |

---

## üîç Notes Importantes

1. **Architecture unifi√©e** : `UnifiedMetricsCallback` utilise uniquement `logger.record()`, garantissant la synchronisation avec SB3 et l'absence de doublons

2. **Fr√©quences** : 
   - `log_freq` contr√¥le la fr√©quence des m√©triques lourdes dans `UnifiedMetricsCallback`
   - `check_freq` contr√¥le la fr√©quence de `OverfittingGuardCallbackV2`

3. **Performance** : 
   - M√©triques l√©g√®res : Buffer √† chaque step, logg√©es √† `log_freq`
   - M√©triques lourdes (gradients, Q-values) : Calcul√©es uniquement √† `log_freq` (√©vite 10-20% de ralentissement)
   - Les buffers sont limit√©s (deque avec maxlen) pour √©viter OOM

4. **WFO** : Les m√©triques HMM et d'√©valuation utilisent `segment_id` comme step pour cr√©er des courbes par segment.

5. **Monitoring TQC** : Le monitoring des Q-values est int√©gr√© dans `UnifiedMetricsCallback` pour le diagnostic Gamma (essentiel pour d√©tecter si gamma est trop petit)

6. **M√©triques √©pur√©es** : Seules les m√©triques vitales sont logg√©es (signal uniquement, pas de bruit). Les composantes individuelles de p√©nalit√©s sont agr√©g√©es en `total_penalties`.

---

## üìù Migration depuis l'Ancienne Architecture

**Callbacks supprim√©s** :
- ‚ùå `TensorBoardStepCallback` (utilisait SummaryWriter directement, cr√©ait des doublons)
- ‚ùå `StepLoggingCallback` (fonctionnalit√© fusionn√©e dans UnifiedMetricsCallback)
- ‚ùå `DetailTensorboardCallback` (fonctionnalit√© fusionn√©e dans UnifiedMetricsCallback)
- ‚ùå `CurriculumFeesCallback` (d√©j√† obsol√®te, remplac√© par ThreePhaseCurriculumCallback)

**Changements de namespaces** :
- `custom/nav` ‚Üí `portfolio/nav`
- `custom/position` ‚Üí `portfolio/position_pct`
- `custom/max_drawdown` ‚Üí `risk/max_drawdown`
- `internal/reward/pnl_component` ‚Üí `rewards/pnl_component`
- `internal/reward/churn_cost` + `internal/reward/smoothness` + `internal/reward/downside_risk` ‚Üí `rewards/total_penalties` (agr√©g√©)
- `grad/actor_norm` ‚Üí `debug/grad_actor_norm`
- `grad/critic_norm` ‚Üí `debug/grad_critic_norm`
- Nouveau : `debug/q_values_mean`, `debug/q_values_std` (monitoring TQC)

**Compatibilit√©** : La m√©thode `get_training_metrics()` est conserv√©e dans `UnifiedMetricsCallback` pour compatibilit√© avec le code existant.

---

**Derni√®re mise √† jour** : 2026-01-23 (Migration vers architecture unifi√©e)
